import os
import pandas as pd
import json
from pathlib import Path
from typing import Dict, List, Tuple, Set, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from datetime import datetime
import time
import traceback
import re
from tqdm import tqdm
from collections import defaultdict

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/root/workspace/video_annotation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class VideoAnnotationProcessor:
    def __init__(self, excel_path: str, video_base_dir: str, output_base_dir: str):
        self.excel_path = excel_path
        self.video_base_dir = video_base_dir
        self.output_base_dir = output_base_dir
        self.annotations = {}  # æŒ‰ç±»åˆ«å­˜å‚¨æ ‡æ³¨
        self.processed_annotations = set()  # è®°å½•å·²å¤„ç†çš„æ ‡æ³¨
        self.video_mapping = {}  # è®°å½•BOSè·¯å¾„åˆ°æœ¬åœ°è·¯å¾„çš„æ˜ å°„
        self.video_stats = {}  # è§†é¢‘ç»Ÿè®¡ä¿¡æ¯
    
    def load_excel_data(self) -> pd.DataFrame:
        """åŠ è½½Excelæ•°æ®"""
        try:
            df = pd.read_excel(self.excel_path)
            logger.info(f"Excelæ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
            
            # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
            required_columns = ['clipè§†é¢‘è·¯å¾„', 'æ ‡ç­¾', 'T_start', 'T_end']
            missing_cols = [col for col in required_columns if col not in df.columns]
            
            if missing_cols:
                raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            
            # æ¸…ç†æ•°æ®ï¼šç§»é™¤è·¯å¾„ä¸­çš„æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
            if 'clipè§†é¢‘è·¯å¾„' in df.columns:
                # è®°å½•æ¸…ç†å‰çš„ç¤ºä¾‹
                sample_before = df['clipè§†é¢‘è·¯å¾„'].iloc[0] if len(df) > 0 else ""
                
                # æ¸…ç†æ¢è¡Œç¬¦å’Œç©ºæ ¼
                df['clipè§†é¢‘è·¯å¾„'] = df['clipè§†é¢‘è·¯å¾„'].astype(str).apply(
                    lambda x: x.strip().replace('\n', '').replace('\r', '').replace('\t', ' ')
                )
                
                # è®°å½•æ¸…ç†åçš„ç¤ºä¾‹
                sample_after = df['clipè§†é¢‘è·¯å¾„'].iloc[0] if len(df) > 0 else ""
                
                logger.info(f"æ¸…ç†è·¯å¾„ä¸­çš„æ¢è¡Œç¬¦: '{sample_before[:50]}...' -> '{sample_after[:50]}...'")
                
                # ç»Ÿè®¡æ¸…ç†æƒ…å†µ
                paths_with_newlines = df['clipè§†é¢‘è·¯å¾„'].astype(str).apply(lambda x: '\n' in x or '\r' in x)
                if paths_with_newlines.any():
                    logger.warning(f"å‘ç° {paths_with_newlines.sum()} ä¸ªè·¯å¾„åŒ…å«æ¢è¡Œç¬¦")
                    for idx, (_, row) in enumerate(df[paths_with_newlines].iterrows()):
                        if idx < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªç¤ºä¾‹
                            logger.warning(f"ç¬¬{row.name}è¡Œ: åŸå§‹è·¯å¾„åŒ…å«æ¢è¡Œç¬¦: {repr(row['clipè§†é¢‘è·¯å¾„'])}")
            
            # æ¸…ç†æ ‡ç­¾åˆ—
            if 'æ ‡ç­¾' in df.columns:
                df['æ ‡ç­¾'] = df['æ ‡ç­¾'].astype(str).str.strip()
            
            return df
            
        except Exception as e:
            logger.error(f"åŠ è½½Excelå¤±è´¥: {str(e)}")
            raise
    
    def bos_to_local_path(self, bos_path: str) -> str:
        """
        å°†BOSè·¯å¾„è½¬æ¢ä¸ºæœ¬åœ°è·¯å¾„
        ç²¾ç¡®åŒ¹é…ï¼šä¸è„šæœ¬1çš„ä¿å­˜è·¯å¾„å®Œå…¨ä¸€è‡´
        """
        try:
            # ç§»é™¤å¼€å¤´çš„bos:å‰ç¼€
            if bos_path.startswith("bos:"):
                bos_path = bos_path[4:]
            
            # ç§»é™¤å¼€å¤´çš„æ–œæ 
            bos_path = bos_path.lstrip('/')
            
            # å»æ‰'neolix-raw/'å‰ç¼€
            if bos_path.startswith("neolix-raw/"):
                bos_path = bos_path[len("neolix-raw/"):]
            
            # ç¡®ä¿è·¯å¾„ä»¥/ç»“å°¾
            if not bos_path.endswith('/'):
                bos_path += '/'
            
            # æ·»åŠ video.mp4
            bos_path += "video.mp4"
            
            # æ„å»ºæœ¬åœ°è·¯å¾„
            local_path = os.path.join(self.video_base_dir, bos_path)
            
            return local_path
            
        except Exception as e:
            logger.error(f"è§£æè·¯å¾„å¤±è´¥ {bos_path}: {str(e)}")
            return None
    
    def find_exact_match(self, bos_path: str) -> Optional[str]:
        """
        ç²¾ç¡®åŒ¹é…BOSè·¯å¾„
        åªæ¥å—å®Œå…¨åŒ¹é…ï¼Œä¸æ¥å—ä»»ä½•æ¨¡ç³ŠåŒ¹é…
        """
        local_path = self.bos_to_local_path(bos_path)
        
        if not local_path:
            return None
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(local_path):
            return local_path
        
        return None
    
    def _create_safe_filename(self, text: str) -> str:
        """åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å"""
        # æ›¿æ¢éæ³•å­—ç¬¦
        safe = re.sub(r'[<>:"/\\|?*]', '_', text)
        safe = re.sub(r'\s+', '_', safe)
        safe = safe.strip('._')
        
        # é™åˆ¶é•¿åº¦
        if len(safe) > 100:
            safe = safe[:100]
        
        return safe
    
    def _create_annotation_id(self, bos_path: str, t_start: int, t_end: int, idx: int) -> str:
        """åˆ›å»ºæ ‡æ³¨ID"""
        import hashlib
        # ä½¿ç”¨BOSè·¯å¾„å’Œæ—¶é—´æˆ³åˆ›å»ºå“ˆå¸Œ
        hash_input = f"{bos_path}_{t_start}_{t_end}_{idx}".encode('utf-8')
        hash_str = hashlib.md5(hash_input).hexdigest()[:8]
        
        # ä»BOSè·¯å¾„ä¸­æå–æœ‰ç”¨ä¿¡æ¯
        date_match = re.search(r"(\d{4}-\d{2}-\d{2}_\d{2}-\d{2}-\d{2})", bos_path)
        date_str = date_match.group(1) if date_match else "unknown"
        
        # åˆ›å»ºID
        annotation_id = f"anno_{idx:04d}_{date_str}_{t_start}s_{t_end}s_{hash_str}"
        return annotation_id
    
    def process_single_row(self, row: pd.Series, idx: int) -> Tuple[bool, str]:
        """å¤„ç†å•è¡Œæ•°æ®"""
        try:
            # è·å–åŸå§‹è§†é¢‘è·¯å¾„
            bos_path = str(row['clipè§†é¢‘è·¯å¾„']).strip()
            # é¢å¤–çš„æ¸…ç†ï¼šç¡®ä¿ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦
            bos_path = re.sub(r'\s+', '', bos_path)  # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€æ¢è¡Œã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰

            label = str(row['æ ‡ç­¾']).strip()
            
            # éªŒè¯æ•°æ®
            if pd.isna(bos_path) or bos_path == '':
                return False, f"ç¬¬{idx}è¡Œ: clipè§†é¢‘è·¯å¾„ä¸ºç©º"
            
            if pd.isna(label) or label == '':
                return False, f"ç¬¬{idx}è¡Œ: æ ‡ç­¾ä¸ºç©º"
            
            # æ£€æŸ¥T_startå’ŒT_endæ˜¯å¦ä¸ºNaN
            if pd.isna(row['T_start']) or pd.isna(row['T_end']):
                return False, f"ç¬¬{idx}è¡Œ: æ—¶é—´æˆ³ä¸ºNaN"
            
            t_start = int(row['T_start'])
            t_end = int(row['T_end'])
            
            if t_start < 0 or t_end < 0 or t_start >= t_end:
                return False, f"ç¬¬{idx}è¡Œ: æ—¶é—´èŒƒå›´æ— æ•ˆ ({t_start}-{t_end})"
            
            # è·å–ç²¾ç¡®åŒ¹é…çš„è§†é¢‘è·¯å¾„
            if bos_path in self.video_mapping:
                local_video_path = self.video_mapping[bos_path]
                video_exists = True
            else:
                local_video_path = self.find_exact_match(bos_path)
                if local_video_path:
                    self.video_mapping[bos_path] = local_video_path
                    video_exists = True
                else:
                    # å°è¯•ç›´æ¥æŸ¥æ‰¾
                    local_path = self.bos_to_local_path(bos_path)
                    logger.warning(f"ç¬¬{idx}è¡Œ: è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼ŒBOSè·¯å¾„: {bos_path}")
                    logger.warning(f"ç¬¬{idx}è¡Œ: æœŸæœ›çš„æœ¬åœ°è·¯å¾„: {local_path}")
                    video_exists = False
            
            # ç”Ÿæˆæ ‡æ³¨ID
            annotation_id = self._create_annotation_id(bos_path, t_start, t_end, idx)
            
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
            if annotation_id in self.processed_annotations:
                logger.debug(f"è·³è¿‡å·²å¤„ç†çš„æ ‡æ³¨: {annotation_id}")
                return True, f"å·²å¤„ç†: {annotation_id}"
            
            # åˆ›å»ºå®‰å…¨çš„ç±»åˆ«åç§°
            safe_label = self._create_safe_filename(label)
            
            # æ·»åŠ åˆ°æ ‡æ³¨
            if safe_label not in self.annotations:
                self.annotations[safe_label] = []
            
            annotation = {
                "id": annotation_id,
                "original_video": local_video_path if video_exists else None,
                "original_bos_path": bos_path,
                "label": label,
                "time_range": [t_start, t_end],
                "duration": t_end - t_start,
                "frame_count": int((t_end - t_start) * 30),  # å‡è®¾30fps
                "source_row": idx,
                "video_exists": video_exists,
                "file_size": os.path.getsize(local_video_path) if video_exists and local_video_path and os.path.exists(local_video_path) else 0
            }
            
            self.annotations[safe_label].append(annotation)
            self.processed_annotations.add(annotation_id)
            
            return True, f"æˆåŠŸæ·»åŠ æ ‡æ³¨: {annotation_id}"
                
        except Exception as e:
            logger.error(f"å¤„ç†è¡Œ {idx} å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            return False, f"å¼‚å¸¸: {str(e)}"
    
    def analyze_data_statistics(self, df: pd.DataFrame) -> Dict:
        """åˆ†ææ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_rows": len(df),
            "valid_rows": 0,
            "invalid_rows": 0,
            "invalid_reasons": defaultdict(int),
            "unique_videos": [],  # ä½¿ç”¨åˆ—è¡¨è€Œä¸æ˜¯é›†åˆ
            "labels_count": {},
            "video_existence": {"exists": 0, "not_exists": 0, "details": []},
            "time_range_stats": {
                "total_duration": 0,
                "avg_duration": 0,
                "min_duration": float('inf'),
                "max_duration": 0
            }
        }
        
        for idx, row in df.iterrows():
            try:
                bos_path = str(row['clipè§†é¢‘è·¯å¾„']).strip()
                label = str(row['æ ‡ç­¾']).strip()
                
                # æ£€æŸ¥å¿…è¦å­—æ®µ
                if pd.isna(bos_path) or bos_path == '':
                    stats["invalid_rows"] += 1
                    stats["invalid_reasons"]["ç©ºè§†é¢‘è·¯å¾„"] += 1
                    continue
                
                if pd.isna(label) or label == '':
                    stats["invalid_rows"] += 1
                    stats["invalid_reasons"]["ç©ºæ ‡ç­¾"] += 1
                    continue
                
                # æ£€æŸ¥T_startå’ŒT_endæ˜¯å¦ä¸ºNaN
                if pd.isna(row['T_start']):
                    stats["invalid_rows"] += 1
                    stats["invalid_reasons"]["T_startä¸ºNaN"] += 1
                    continue
                    
                if pd.isna(row['T_end']):
                    stats["invalid_rows"] += 1
                    stats["invalid_reasons"]["T_endä¸ºNaN"] += 1
                    continue
                
                t_start = int(row['T_start'])
                t_end = int(row['T_end'])
                
                if t_start < 0 or t_end < 0:
                    stats["invalid_rows"] += 1
                    stats["invalid_reasons"]["æ—¶é—´æˆ³ä¸ºè´Ÿ"] += 1
                    continue
                
                if t_start >= t_end:
                    stats["invalid_rows"] += 1
                    stats["invalid_reasons"]["å¼€å§‹æ—¶é—´å¤§äºç­‰äºç»“æŸæ—¶é—´"] += 1
                    continue
                
                # æœ‰æ•ˆçš„è¡Œ
                stats["valid_rows"] += 1
                
                # ç»Ÿè®¡å”¯ä¸€è§†é¢‘
                if bos_path not in stats["unique_videos"]:
                    stats["unique_videos"].append(bos_path)
                
                # ç»Ÿè®¡æ ‡ç­¾
                if label in stats["labels_count"]:
                    stats["labels_count"][label] += 1
                else:
                    stats["labels_count"][label] = 1
                
                # ç»Ÿè®¡æ—¶é—´èŒƒå›´
                duration = t_end - t_start
                stats["time_range_stats"]["total_duration"] += duration
                stats["time_range_stats"]["min_duration"] = min(stats["time_range_stats"]["min_duration"], duration)
                stats["time_range_stats"]["max_duration"] = max(stats["time_range_stats"]["max_duration"], duration)
                
                # æ£€æŸ¥è§†é¢‘æ˜¯å¦å­˜åœ¨
                local_path = self.find_exact_match(bos_path)
                exists = local_path and os.path.exists(local_path)
                
                if exists:
                    stats["video_existence"]["exists"] += 1
                else:
                    stats["video_existence"]["not_exists"] += 1
                
                stats["video_existence"]["details"].append({
                    "row": idx,
                    "bos_path": bos_path,
                    "local_path": local_path,
                    "exists": exists,
                    "label": label
                })
                        
            except Exception as e:
                logger.debug(f"åˆ†æè¡Œ {idx} å¤±è´¥: {str(e)}")
                stats["invalid_rows"] += 1
                stats["invalid_reasons"][str(type(e).__name__)] += 1
                continue
        
        # è®¡ç®—å¹³å‡å€¼
        if stats["valid_rows"] > 0:
            stats["time_range_stats"]["avg_duration"] = stats["time_range_stats"]["total_duration"] / stats["valid_rows"]
        
        stats["unique_videos_count"] = len(stats["unique_videos"])
        
        return stats
    
    def save_annotations(self):
        """ä¿å­˜æ ‡æ³¨åˆ°JSONæ–‡ä»¶"""
        output_dir = os.path.join(self.output_base_dir, "annotations")
        os.makedirs(output_dir, exist_ok=True)
        
        total_annotations = 0
        category_stats = {}
        
        # ä¸ºæ¯ä¸ªç±»åˆ«ä¿å­˜å•ç‹¬çš„JSONæ–‡ä»¶
        for label, annotations in self.annotations.items():
            if not annotations:
                continue
                
            # åˆ›å»ºå®‰å…¨æ–‡ä»¶å
            safe_label_name = self._create_safe_filename(label)
            json_path = os.path.join(output_dir, f"{safe_label_name}.json")
            
            # ä¸ºæ¯ä¸ªæ ‡æ³¨æ·»åŠ ç´¢å¼•
            for i, anno in enumerate(annotations):
                anno["index_in_category"] = i
            
            # ä¿å­˜ä¸ºJSON
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(annotations, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ä¿å­˜æ ‡æ³¨æ–‡ä»¶: {json_path} ({len(annotations)} ä¸ªæ ‡æ³¨)")
            total_annotations += len(annotations)
            category_stats[label] = len(annotations)
        
        # ä¿å­˜æ±‡æ€»æ–‡ä»¶
        summary = {
            "total_categories": len(self.annotations),
            "total_annotations": total_annotations,
            "annotations_per_category": category_stats,
            "categories": list(self.annotations.keys()),
            "processing_time": datetime.now().isoformat()
        }
        
        summary_path = os.path.join(output_dir, "summary.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜æ±‡æ€»æ–‡ä»¶: {summary_path}")
        
        # ä¿å­˜åˆå¹¶çš„æ‰€æœ‰æ ‡æ³¨
        all_annotations = []
        for label, annotations in self.annotations.items():
            all_annotations.extend(annotations)
        
        all_annotations_path = os.path.join(output_dir, "all_annotations.json")
        with open(all_annotations_path, 'w', encoding='utf-8') as f:
            json.dump(all_annotations, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜åˆå¹¶æ ‡æ³¨æ–‡ä»¶: {all_annotations_path} ({len(all_annotations)} ä¸ªæ ‡æ³¨)")
        
        return total_annotations
    
    def save_video_mapping(self):
        """ä¿å­˜è§†é¢‘è·¯å¾„æ˜ å°„"""
        mapping_path = os.path.join(self.output_base_dir, "video_mapping.json")
        
        mapping_data = {
            "total_mappings": len(self.video_mapping),
            "mappings": [
                {
                    "bos_path": bos_path,
                    "local_path": local_path,
                    "exists": os.path.exists(local_path) if local_path else False
                }
                for bos_path, local_path in self.video_mapping.items()
            ],
            "processing_time": datetime.now().isoformat()
        }
        
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump(mapping_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜è§†é¢‘è·¯å¾„æ˜ å°„: {mapping_path}")
    
    def save_statistics(self, success_count: int, fail_count: int, 
                       data_stats: Dict, fail_details: List, elapsed_time: float):
        """ä¿å­˜è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "processing_summary": {
                "total_processed": success_count + fail_count,
                "success_count": success_count,
                "fail_count": fail_count,
                "success_rate": success_count / (success_count + fail_count) if (success_count + fail_count) > 0 else 0,
                "categories_created": len(self.annotations),
                "annotations_created": sum(len(annos) for annos in self.annotations.values()),
                "processing_time": datetime.now().isoformat(),
                "duration_seconds": elapsed_time
            },
            "data_statistics": data_stats,
            "fail_details": [
                {"row": idx, "reason": reason} for idx, reason in fail_details[:100]
            ],
            "configuration": {
                "excel_path": self.excel_path,
                "video_base_dir": self.video_base_dir,
                "output_base_dir": self.output_base_dir
            }
        }
        
        stats_path = os.path.join(self.output_base_dir, "processing_statistics.json")
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_path}")
        return stats
    
    def process_all(self, max_workers: int = 4):
        """å¤„ç†æ‰€æœ‰æ•°æ®"""
        # åŠ è½½æ•°æ®
        df = self.load_excel_data()
        
        # åˆ†ææ•°æ®ç»Ÿè®¡
        logger.info("åˆ†ææ•°æ®ç»Ÿè®¡...")
        stats = self.analyze_data_statistics(df)
        
        print("\n" + "=" * 60)
        print("ğŸ“Š æ ‡æ³¨æ•°æ®ç»Ÿè®¡:")
        print("=" * 60)
        print(f"ğŸ“„ åŸå§‹æ ‡æ³¨è¡Œæ•°: {stats['total_rows']}")
        print(f"âœ… æœ‰æ•ˆæ ‡æ³¨æ•°: {stats['valid_rows']}")
        print(f"âŒ æ— æ•ˆæ ‡æ³¨æ•°: {stats['invalid_rows']}")
        
        if stats['invalid_reasons']:
            print(f"ğŸ“‰ æ— æ•ˆåŸå› ç»Ÿè®¡:")
            for reason, count in sorted(stats['invalid_reasons'].items(), key=lambda x: x[1], reverse=True):
                print(f"  - {reason}: {count}")
        
        print(f"ğŸ“ å”¯ä¸€BOSè§†é¢‘è·¯å¾„: {stats['unique_videos_count']}")
        
        # æ˜¾ç¤ºæ ‡ç­¾ç»Ÿè®¡
        if stats['labels_count']:
            print(f"\nğŸ“‚ æŒ‰ç±»åˆ«ç»Ÿè®¡:")
            sorted_labels = sorted(stats['labels_count'].items(), key=lambda x: x[1], reverse=True)
            for label, count in sorted_labels[:20]:
                print(f"  - {label}: {count} ä¸ªæ ‡æ³¨")
            if len(sorted_labels) > 20:
                print(f"  ... è¿˜æœ‰ {len(sorted_labels) - 20} ä¸ªç±»åˆ«")
        
        # æ˜¾ç¤ºæ—¶é—´ç»Ÿè®¡
        print(f"\nâ±ï¸  æ—¶é—´èŒƒå›´ç»Ÿè®¡:")
        print(f"  - æ€»æ—¶é•¿: {stats['time_range_stats']['total_duration']} ç§’")
        print(f"  - å¹³å‡æ—¶é•¿: {stats['time_range_stats']['avg_duration']:.2f} ç§’")
        print(f"  - æœ€çŸ­æ—¶é•¿: {stats['time_range_stats']['min_duration']} ç§’")
        print(f"  - æœ€é•¿æ—¶é•¿: {stats['time_range_stats']['max_duration']} ç§’")
        
        print(f"\nğŸ“¹ è§†é¢‘æ–‡ä»¶åŒ¹é…æƒ…å†µ:")
        print(f"  âœ… å¯æ‰¾åˆ°çš„è§†é¢‘: {stats['video_existence']['exists']}")
        print(f"  âŒ ç¼ºå¤±çš„è§†é¢‘: {stats['video_existence']['not_exists']}")
        
        if stats['video_existence']['not_exists'] > 0:
            print(f"\nâš ï¸  è­¦å‘Š: æœ‰ {stats['video_existence']['not_exists']} ä¸ªæ ‡æ³¨æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è§†é¢‘!")
            print("å¯èƒ½çš„åŸå› :")
            print("1. è§†é¢‘æœªä¸‹è½½æˆ–ä¸‹è½½ä¸å®Œæ•´")
            print("2. Excelä¸­çš„è·¯å¾„ä¸ä¸‹è½½çš„è§†é¢‘è·¯å¾„ä¸åŒ¹é…")
            print("3. è§†é¢‘æ–‡ä»¶åä¸æ­£ç¡®")
            
            # æ˜¾ç¤ºä¸åŒ¹é…çš„è¯¦ç»†ä¿¡æ¯
            not_exists_details = [d for d in stats['video_existence']['details'] if not d['exists']]
            if not_exists_details:
                print(f"\nğŸ” ä¸åŒ¹é…çš„ç¤ºä¾‹ (å‰5ä¸ª):")
                for i, detail in enumerate(not_exists_details[:5], 1):
                    print(f"\n  {i}. è¡Œ {detail['row']}:")
                    print(f"     æ ‡ç­¾: {detail['label']}")
                    print(f"     BOSè·¯å¾„: {detail['bos_path']}")
                    print(f"     æœŸæœ›çš„æœ¬åœ°è·¯å¾„: {detail['local_path']}")
        
        if stats['valid_rows'] == 0:
            logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ ‡æ³¨æ•°æ®ï¼")
            return 0, 0
        
        logger.info(f"å¼€å§‹å¤„ç† {len(df)} ä¸ªæ ‡æ³¨è¡Œ...")
        
        # å‡†å¤‡å¤„ç†ä»»åŠ¡
        tasks = []
        for idx, row in df.iterrows():
            tasks.append((idx, row))
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        success_count = 0
        fail_count = 0
        fail_details = []
        
        with tqdm(total=len(tasks), desc="å¤„ç†è¿›åº¦", unit="è¡Œ") as pbar:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_idx = {}
                for idx, row in tasks:
                    future = executor.submit(self.process_single_row, row, idx)
                    future_to_idx[future] = idx
                
                # å¤„ç†ç»“æœ
                for future in as_completed(future_to_idx):
                    idx = future_to_idx[future]
                    try:
                        success, message = future.result(timeout=30)
                        if success:
                            success_count += 1
                        else:
                            fail_count += 1
                            fail_details.append((idx, message))
                    except Exception as e:
                        fail_count += 1
                        fail_details.append((idx, f"å¤„ç†å¼‚å¸¸: {str(e)}"))
                        logger.error(f"è¡Œ {idx} å¤„ç†å¼‚å¸¸: {str(e)}")
                    
                    pbar.update(1)
                    pbar.set_postfix_str(f"æˆåŠŸ: {success_count}, å¤±è´¥: {fail_count}")
        
        return success_count, fail_count, stats, fail_details

def verify_downloaded_videos(video_base_dir: str):
    """éªŒè¯ä¸‹è½½çš„è§†é¢‘æ–‡ä»¶"""
    print("\n" + "=" * 60)
    print("éªŒè¯ä¸‹è½½çš„è§†é¢‘æ–‡ä»¶...")
    print("=" * 60)
    
    if not os.path.exists(video_base_dir):
        print(f"âŒ è§†é¢‘ç›®å½•ä¸å­˜åœ¨: {video_base_dir}")
        return 0
    
    # æŸ¥æ‰¾æ‰€æœ‰è§†é¢‘æ–‡ä»¶
    video_files = []
    video_sizes = {}
    
    for root, dirs, files in os.walk(video_base_dir):
        for file in files:
            if file.endswith('.mp4'):
                video_path = os.path.join(root, file)
                rel_path = os.path.relpath(video_path, video_base_dir)
                file_size = os.path.getsize(video_path) / (1024 * 1024)  # MB
                video_files.append(rel_path)
                video_sizes[rel_path] = file_size
    
    print(f"æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
    
    if video_files:
        # è®¡ç®—æ€»å¤§å°
        total_size = sum(video_sizes.values())
        avg_size = total_size / len(video_files) if video_files else 0
        
        print(f"æ€»å¤§å°: {total_size:.2f} MB")
        print(f"å¹³å‡å¤§å°: {avg_size:.2f} MB")
        
        print("\nè§†é¢‘æ–‡ä»¶ç¤ºä¾‹:")
        for i, rel_path in enumerate(video_files[:5], 1):
            size = video_sizes[rel_path]
            print(f"  {i}. {rel_path} ({size:.2f} MB)")
        
        if len(video_files) > 5:
            print(f"  ... è¿˜æœ‰ {len(video_files) - 5} ä¸ªæ–‡ä»¶")
    
    return len(video_files)

def main():
    """ä¸»å‡½æ•°"""
    EXCEL_PATH = "/root/workspace/äººå·¥æ ‡æ³¨è§†é¢‘æ•°æ®_å¯¹æ¯”å®éªŒ_12tags_.xlsx"
    VIDEO_BASE_DIR = "/root/workspace/downloaded_videos_2fps"
    OUTPUT_BASE_DIR = "/root/workspace/vqa_annotations_2fps"
    
    print("=" * 60)
    print("ğŸ¯ è§†é¢‘æ ‡æ³¨ç”Ÿæˆå·¥å…·")
    print("=" * 60)
    print(f"ğŸ“ è§†é¢‘ç›®å½•: {VIDEO_BASE_DIR}")
    print(f"ğŸ“„ Excelæ–‡ä»¶: {EXCEL_PATH}")
    print(f"ğŸ“¦ è¾“å‡ºç›®å½•: {OUTPUT_BASE_DIR}")
    print("=" * 60)
    print("ğŸ“‹ åŠŸèƒ½è¯´æ˜:")
    print("  - è¯»å–Excelä¸­çš„è§†é¢‘æ ‡æ³¨æ•°æ®")
    print("  - å°†BOSè·¯å¾„è½¬æ¢ä¸ºæœ¬åœ°è·¯å¾„")
    print("  - æŒ‰ç±»åˆ«ç”Ÿæˆæ ‡æ³¨æ–‡æ¡£")
    print("  - ä¸è¿›è¡Œè§†é¢‘åˆ‡åˆ†ï¼Œä¿ç•™åŸå§‹è§†é¢‘")
    print("=" * 60)
    
    # éªŒè¯ä¸‹è½½çš„è§†é¢‘æ–‡ä»¶
    video_count = verify_downloaded_videos(VIDEO_BASE_DIR)
    if video_count == 0:
        logger.warning("æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶ï¼Œä½†æ ‡æ³¨ç”Ÿæˆå°†ç»§ç»­è¿›è¡Œ")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(EXCEL_PATH):
        logger.error(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {EXCEL_PATH}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = VideoAnnotationProcessor(
        excel_path=EXCEL_PATH,
        video_base_dir=VIDEO_BASE_DIR,
        output_base_dir=OUTPUT_BASE_DIR
    )
    
    # å¤„ç†æ‰€æœ‰æ•°æ®
    start_time = time.time()
    success_count, fail_count, data_stats, fail_details = processor.process_all(max_workers=4)
    elapsed_time = time.time() - start_time
    
    # ä¿å­˜ç»“æœ
    if processor.annotations:
        total_annotations = processor.save_annotations()
        processor.save_video_mapping()
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = processor.save_statistics(success_count, fail_count, data_stats, fail_details, elapsed_time)
    else:
        total_annotations = 0
        logger.warning("æ²¡æœ‰ç”Ÿæˆä»»ä½•æ ‡æ³¨æ•°æ®")
        stats = processor.save_statistics(success_count, fail_count, data_stats, fail_details, elapsed_time)
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ‰ æ ‡æ³¨ç”Ÿæˆå®Œæˆ")
    print("=" * 60)
    print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
    print(f"ğŸ“Š æ€»è®¡å¤„ç†: {success_count + fail_count} è¡Œ")
    print(f"âœ… æˆåŠŸ: {success_count} è¡Œ")
    print(f"âŒ å¤±è´¥: {fail_count} è¡Œ")
    
    if success_count > 0:
        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_BASE_DIR}")
        print("ç›®å½•ç»“æ„:")
        print(f"  {OUTPUT_BASE_DIR}/")
        print(f"  â”œâ”€â”€ annotations/             # æ ‡æ³¨æ–‡ä»¶")
        print(f"  â”‚   â”œâ”€â”€ all_annotations.json  # æ‰€æœ‰æ ‡æ³¨çš„åˆå¹¶æ–‡ä»¶")
        print(f"  â”‚   â”œâ”€â”€ summary.json         # æ±‡æ€»ä¿¡æ¯")
        print(f"  â”‚   â””â”€â”€ [ç±»åˆ«].json         # æ¯ä¸ªç±»åˆ«çš„æ ‡æ³¨")
        print(f"  â”œâ”€â”€ video_mapping.json       # è§†é¢‘è·¯å¾„æ˜ å°„")
        print(f"  â””â”€â”€ processing_statistics.json  # å¤„ç†ç»Ÿè®¡")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„ç±»åˆ«
        if processor.annotations:
            print(f"\nğŸ“‚ ç”Ÿæˆçš„æ ‡æ³¨ç±»åˆ« ({len(processor.annotations)} ä¸ª):")
            for label, annotations in sorted(processor.annotations.items(), 
                                          key=lambda x: len(x[1]), reverse=True)[:10]:
                print(f"  - {label}: {len(annotations)} ä¸ªæ ‡æ³¨")
            if len(processor.annotations) > 10:
                print(f"  ... è¿˜æœ‰ {len(processor.annotations) - 10} ä¸ªç±»åˆ«")
        
        print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥:")
        print("1. æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶: ls -la /root/workspace/vqa_annotations/annotations/")
        print("2. æŸ¥çœ‹æ ‡æ³¨ç»Ÿè®¡: cat /root/workspace/vqa_annotations/processing_statistics.json | python -m json.tool")
        print("3. ä½¿ç”¨æ ‡æ³¨æ–‡ä»¶è¿›è¡Œæ¨¡å‹è®­ç»ƒ")
        
        # æ˜¾ç¤ºæ ‡æ³¨ç¤ºä¾‹
        print(f"\nğŸ“ æ ‡æ³¨ç¤ºä¾‹:")
        for label, annotations in sorted(processor.annotations.items(), 
                                      key=lambda x: len(x[1]), reverse=True):
            if annotations:
                anno = annotations[0]
                print(f"  ç±»åˆ«: {label}")
                print(f"    è§†é¢‘: {os.path.basename(anno.get('original_video', 'N/A'))}")
                print(f"    æ—¶é—´èŒƒå›´: {anno['time_range'][0]}s - {anno['time_range'][1]}s")
                print(f"    æ—¶é•¿: {anno['duration']}s")
                break
    else:
        print(f"\nâŒ å¤„ç†å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆä»»ä½•æ ‡æ³¨æ•°æ®")
        print("å¯èƒ½çš„åŸå› :")
        print("1. Excelæ–‡ä»¶æ ¼å¼é”™è¯¯")
        print("2. æ‰€æœ‰è¡Œéƒ½æœ‰æ•°æ®é—®é¢˜")
        print("3. æ²¡æœ‰æœ‰æ•ˆçš„æ ‡æ³¨è¡Œ")
        print(f"\nğŸ” æŸ¥çœ‹è¯¦ç»†æ—¥å¿—: tail -100 /root/workspace/video_annotation.log")
    
    print("=" * 60)

if __name__ == "__main__":
    main()