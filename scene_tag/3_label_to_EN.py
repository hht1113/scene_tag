import os
import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from datetime import datetime
import time
import traceback
from tqdm import tqdm
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/root/workspace/label_conversion.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class LabelConverter:
    """æ ‡ç­¾è½¬æ¢å™¨ï¼Œå¤„ç†ä¸­è‹±æ–‡æ ‡ç­¾æ˜ å°„ï¼ˆä»…ç²¾ç¡®åŒ¹é…ï¼‰"""
    
    def __init__(self, mapping_file: str):
        self.mapping_file = mapping_file
        self.chinese_to_english = {}
        self.english_to_chinese = {}
        self.unmapped_labels = set()
        self.load_mapping()
    
    def load_mapping(self):
        """åŠ è½½ä¸­è‹±æ–‡å¯¹ç…§è¡¨ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰"""
        try:
            if not os.path.exists(self.mapping_file):
                logger.warning(f"ä¸­è‹±æ–‡å¯¹ç…§è¡¨ä¸å­˜åœ¨: {self.mapping_file}")
                return
            
            df = pd.read_excel(self.mapping_file)
            logger.info(f"åŠ è½½ä¸­è‹±æ–‡å¯¹ç…§è¡¨: {len(df)} è¡Œ")
            
            # ç¡®ä¿åˆ—åæ­£ç¡®
            required_columns = ['ä¸­æ–‡æ ‡ç­¾', 'è‹±æ–‡æ ‡ç­¾']
            for col in required_columns:
                if col not in df.columns:
                    logger.error(f"å¯¹ç…§è¡¨ç¼ºå°‘å¿…è¦åˆ—: {col}")
                    return
            
            # æ„å»ºæ˜ å°„
            for _, row in df.iterrows():
                chinese = str(row['ä¸­æ–‡æ ‡ç­¾']).strip()
                english = str(row['è‹±æ–‡æ ‡ç­¾']).strip()
                
                if chinese and english and chinese != 'nan' and english != 'nan':
                    self.chinese_to_english[chinese] = english
                    self.english_to_chinese[english] = chinese
                    logger.debug(f"æ˜ å°„: {chinese} -> {english}")
            
            logger.info(f"åŠ è½½äº† {len(self.chinese_to_english)} ä¸ªæ ‡ç­¾æ˜ å°„")
            
        except Exception as e:
            logger.error(f"åŠ è½½æ ‡ç­¾æ˜ å°„å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
    
    def convert_label(self, chinese_label: str) -> Tuple[str, bool]:
        """
        è½¬æ¢ä¸­æ–‡æ ‡ç­¾ä¸ºè‹±æ–‡æ ‡ç­¾
        è¿”å›: (è‹±æ–‡æ ‡ç­¾, æ˜¯å¦æˆåŠŸæ˜ å°„)
        åªè¿›è¡Œç²¾ç¡®åŒ¹é…ï¼Œä¸åŒ¹é…åˆ™è¿”å›åŸä¸­æ–‡æ ‡ç­¾
        """
        if not chinese_label:
            return "", False
        
        # ä»…å°è¯•å®Œå…¨åŒ¹é…
        if chinese_label in self.chinese_to_english:
            return self.chinese_to_english[chinese_label], True
        else:
            # è®°å½•æœªæ˜ å°„çš„æ ‡ç­¾
            self.unmapped_labels.add(chinese_label)
            logger.warning(f"æœªæ‰¾åˆ°æ˜ å°„çš„æ ‡ç­¾: {chinese_label}")
            # è¿”å›åŸä¸­æ–‡æ ‡ç­¾
            return chinese_label, False

class AnnotationProcessor:
    """æ ‡æ³¨å¤„ç†å™¨ï¼Œè½¬æ¢æ ‡ç­¾å¹¶å‡†å¤‡è§†é¢‘VQAå¾®è°ƒæ•°æ®"""
    
    def __init__(self, annotations_dir: str, mapping_file: str, output_dir: str):
        self.annotations_dir = annotations_dir
        self.mapping_file = mapping_file
        self.output_dir = output_dir
        self.label_converter = LabelConverter(mapping_file)
        
    def process_all_annotations(self) -> Dict:
        """å¤„ç†æ‰€æœ‰æ ‡æ³¨æ–‡ä»¶"""
        # æŸ¥æ‰¾æ‰€æœ‰æ ‡æ³¨æ–‡ä»¶
        annotation_files = []
        for file in os.listdir(self.annotations_dir):
            if file.endswith('.json') and file != 'summary.json':
                annotation_files.append(os.path.join(self.annotations_dir, file))
        
        logger.info(f"æ‰¾åˆ° {len(annotation_files)} ä¸ªæ ‡æ³¨æ–‡ä»¶")
        
        all_converted_data = []
        category_stats = {}
        
        for annotation_file in tqdm(annotation_files, desc="å¤„ç†æ ‡æ³¨æ–‡ä»¶"):
            category_name = os.path.basename(annotation_file).replace('.json', '')
            category_data = self.process_single_file(annotation_file, category_name)
            
            if category_data:
                category_stats[category_name] = len(category_data)
                all_converted_data.extend(category_data)
        
        # ä¿å­˜ç»“æœ
        self.save_results(all_converted_data, category_stats)
        
        return {
            "total_samples": len(all_converted_data),
            "category_stats": category_stats,
            "unmapped_labels": list(self.label_converter.unmapped_labels)
        }
    
    def process_single_file(self, annotation_file: str, category_name: str) -> List[Dict]:
        """å¤„ç†å•ä¸ªæ ‡æ³¨æ–‡ä»¶"""
        try:
            with open(annotation_file, 'r', encoding='utf-8') as f:
                annotations = json.load(f)
            
            converted_annotations = []
            
            for ann in tqdm(annotations, desc=f"å¤„ç† {category_name}", leave=False):
                converted = self.process_single_annotation(ann)
                if converted:
                    converted_annotations.append(converted)
            
            return converted_annotations
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {annotation_file}: {str(e)}")
            logger.error(traceback.format_exc())
            return []
    
    def process_single_annotation(self, annotation: Dict) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªæ ‡æ³¨"""
        try:
            chinese_label = annotation.get('label', '')
            
            if not chinese_label:
                logger.warning(f"è·³è¿‡æ— æ•ˆæ ‡æ³¨: {annotation.get('id', 'unknown')}")
                return None
            
            # è½¬æ¢æ ‡ç­¾
            english_label, mapped = self.label_converter.convert_label(chinese_label)
            
            # è·å–è§†é¢‘è·¯å¾„
            video_path = annotation.get('original_video', '')
            bos_path = annotation.get('original_bos_path', '')
            
            # å¦‚æœåŸå§‹è§†é¢‘è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨ç›¸å¯¹è·¯å¾„
            if not video_path or not os.path.exists(video_path):
                if bos_path:
                    # ä»BOSè·¯å¾„æ„é€ å¯èƒ½çš„æœ¬åœ°è·¯å¾„
                    video_path = self._bos_to_local_path(bos_path)
                else:
                    video_path = annotation.get('video_path', '')
            
            # éªŒè¯è§†é¢‘æ˜¯å¦å­˜åœ¨
            video_exists = os.path.exists(video_path) if video_path else False
            if not video_exists and video_path:
                logger.warning(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            
            # å‡†å¤‡è¾“å‡ºæ•°æ®
            result = {
                "id": annotation.get('id', ''),
                "video_path": video_path,
                "video_exists": video_exists,
                "label_zh": chinese_label,
                "label_en": english_label,
                "label_mapped": mapped,
                "time_range": annotation.get('time_range', []),
                "duration": annotation.get('duration', 0),
                "original_info": {
                    "original_video": annotation.get('original_video', ''),
                    "original_bos_path": annotation.get('original_bos_path', ''),
                    "source_row": annotation.get('source_row', 0)
                }
            }
            
            # æ·»åŠ è§†é¢‘æ–‡ä»¶ä¿¡æ¯
            if video_exists:
                try:
                    file_size = os.path.getsize(video_path)
                    result["video_size"] = file_size
                    result["video_size_mb"] = file_size / (1024 * 1024)
                except:
                    result["video_size"] = 0
                    result["video_size_mb"] = 0
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†æ ‡æ³¨å¤±è´¥ {annotation.get('id', 'unknown')}: {str(e)}")
            logger.error(traceback.format_exc())
            return None
    
    def _bos_to_local_path(self, bos_path: str) -> str:
        """
        å°†BOSè·¯å¾„è½¬æ¢ä¸ºæœ¬åœ°è·¯å¾„
        ä¸ä¹‹å‰è„šæœ¬ä¿æŒä¸€è‡´çš„é€»è¾‘
        """
        try:
            # ç§»é™¤å¼€å¤´çš„bos:å‰ç¼€
            if bos_path.startswith("bos:"):
                bos_path = bos_path[4:]
            
            # ç§»é™¤å¼€å¤´çš„æ–œæ 
            bos_path = bos_path.lstrip('/')
            
            # å»æ‰'neolix-raw/'å‰ç¼€
            if bos_path.startswith("neolix-raw/"):
                bos_path = bos_path[len("neolix-raw/"):]
            
            # ç¡®ä¿è·¯å¾„ä»¥/ç»“å°¾
            if not bos_path.endswith('/'):
                bos_path += '/'
            
            # æ·»åŠ video.mp4
            bos_path += "video.mp4"
            
            # æ„å»ºæœ¬åœ°è·¯å¾„ï¼ˆå‡è®¾è§†é¢‘åœ¨/root/workspace/downloaded_videosç›®å½•ä¸‹ï¼‰
            local_path = os.path.join("/root/workspace/downloaded_videos", bos_path)
            
            return local_path
            
        except Exception as e:
            logger.error(f"è§£æBOSè·¯å¾„å¤±è´¥ {bos_path}: {str(e)}")
            return ""
    
    def save_results(self, all_data: List[Dict], category_stats: Dict):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_annotations_dir = os.path.join(self.output_dir, "converted_annotations")
        os.makedirs(output_annotations_dir, exist_ok=True)
        
        # ç»Ÿè®¡è§†é¢‘å­˜åœ¨æƒ…å†µ
        video_exists_count = sum(1 for item in all_data if item.get("video_exists", False))
        video_missing_count = len(all_data) - video_exists_count
        
        # 1. ä¿å­˜å®Œæ•´æ•°æ®é›†
        output_file = os.path.join(output_annotations_dir, "video_vqa_dataset.json")
        
        dataset = {
            "version": "1.0.0",
            "description": "Video VQA dataset with English labels (no frame extraction)",
            "created": datetime.now().isoformat(),
            "statistics": {
                "total_samples": len(all_data),
                "video_exists": video_exists_count,
                "video_missing": video_missing_count,
                "categories": category_stats,
                "unmapped_labels": list(self.label_converter.unmapped_labels)
            },
            "data": all_data
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜å®Œæ•´æ•°æ®é›†: {output_file} ({len(all_data)} ä¸ªæ ·æœ¬)")
        logger.info(f"è§†é¢‘å­˜åœ¨: {video_exists_count}, è§†é¢‘ç¼ºå¤±: {video_missing_count}")
        
        # 2. æŒ‰ç±»åˆ«ä¿å­˜
        categories_data = {}
        for item in all_data:
            label_zh = item.get("label_zh", "unknown")
            if label_zh not in categories_data:
                categories_data[label_zh] = []
            categories_data[label_zh].append(item)
        
        for label_zh, items in categories_data.items():
            # åˆ›å»ºå®‰å…¨æ–‡ä»¶å
            safe_name = self._create_safe_filename(label_zh)
            category_file = os.path.join(
                output_annotations_dir, f"{safe_name}.json"
            )
            
            # ç»Ÿè®¡è¯¥ç±»åˆ«çš„è§†é¢‘å­˜åœ¨æƒ…å†µ
            category_video_exists = sum(1 for item in items if item.get("video_exists", False))
            
            category_dataset = {
                "label_zh": label_zh,
                "label_en": items[0].get("label_en", "") if items else "",
                "count": len(items),
                "video_exists": category_video_exists,
                "video_missing": len(items) - category_video_exists,
                "data": items
            }
            
            with open(category_file, 'w', encoding='utf-8') as f:
                json.dump(category_dataset, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜äº† {len(categories_data)} ä¸ªç±»åˆ«æ–‡ä»¶")
        
        # 3. ä¿å­˜ç®€åŒ–ç‰ˆæœ¬ï¼ˆç”¨äºåç»­å¤„ç†ï¼‰
        simple_data = []
        for item in all_data:
            simple_item = {
                "id": item.get("id", ""),
                "video_path": item.get("video_path", ""),
                "video_exists": item.get("video_exists", False),
                "label_zh": item.get("label_zh", ""),
                "label_en": item.get("label_en", ""),
                "time_range": item.get("time_range", []),
                "duration": item.get("duration", 0)
            }
            simple_data.append(simple_item)
        
        simple_file = os.path.join(output_annotations_dir, "simple_dataset.json")
        with open(simple_file, 'w', encoding='utf-8') as f:
            json.dump(simple_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜ç®€åŒ–æ•°æ®é›†: {simple_file}")
        
        # 4. ä¿å­˜ä»…åŒ…å«è§†é¢‘å­˜åœ¨çš„æ ·æœ¬
        existing_videos_data = [item for item in all_data if item.get("video_exists", False)]
        existing_file = os.path.join(output_annotations_dir, "existing_videos_dataset.json")
        
        existing_dataset = {
            "version": "1.0.0",
            "description": "Video VQA dataset with existing videos only",
            "created": datetime.now().isoformat(),
            "statistics": {
                "total_samples": len(existing_videos_data),
                "categories": {k: v for k, v in category_stats.items() 
                             if k in {item.get("label_zh") for item in existing_videos_data}}
            },
            "data": existing_videos_data
        }
        
        with open(existing_file, 'w', encoding='utf-8') as f:
            json.dump(existing_dataset, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜ä»…åŒ…å«å­˜åœ¨è§†é¢‘çš„æ•°æ®é›†: {existing_file} ({len(existing_videos_data)} ä¸ªæ ·æœ¬)")
        
        # 5. ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "processing_time": datetime.now().isoformat(),
            "total_samples": len(all_data),
            "video_exists": video_exists_count,
            "video_missing": video_missing_count,
            "categories": category_stats,
            "unmapped_labels": list(self.label_converter.unmapped_labels),
            "unmapped_count": len(self.label_converter.unmapped_labels),
            "mapping_rate": (len(all_data) - len(self.label_converter.unmapped_labels)) / len(all_data) 
                           if len(all_data) > 0 else 0
        }
        
        stats_file = os.path.join(output_annotations_dir, "statistics.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
        
        # 6. ä¿å­˜æ ‡ç­¾æ˜ å°„ç»Ÿè®¡
        label_mapping_stats = []
        for item in all_data:
            label_mapping_stats.append({
                "id": item.get("id", ""),
                "label_zh": item.get("label_zh", ""),
                "label_en": item.get("label_en", ""),
                "mapped": item.get("label_mapped", False)
            })
        
        mapping_stats_file = os.path.join(output_annotations_dir, "label_mapping_stats.json")
        with open(mapping_stats_file, 'w', encoding='utf-8') as f:
            json.dump(label_mapping_stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜æ ‡ç­¾æ˜ å°„ç»Ÿè®¡: {mapping_stats_file}")
    
    def _create_safe_filename(self, text: str) -> str:
        """åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å"""
        # æ›¿æ¢éæ³•å­—ç¬¦
        safe = re.sub(r'[<>:"/\\|?*]', '_', text)
        safe = re.sub(r'\s+', '_', safe)
        safe = safe.strip('._')
        
        # é™åˆ¶é•¿åº¦
        if len(safe) > 100:
            safe = safe[:100]
        
        return safe

def main():
    """ä¸»å‡½æ•°"""
    ANNOTATIONS_DIR = "/root/workspace/vqa_annotations/annotations"
    MAPPING_FILE = "/root/workspace/ä¸­è‹±å¯¹ç…§è¡¨.xlsx"
    OUTPUT_DIR = "/root/workspace/vqa_dataset_prepared"
    
    print("=" * 60)
    print("è§†é¢‘VQAæ ‡æ³¨æ ‡ç­¾è½¬æ¢å·¥å…·")
    print("=" * 60)
    print(f"ğŸ“ æ ‡æ³¨ç›®å½•: {ANNOTATIONS_DIR}")
    print(f"ğŸ“„ ä¸­è‹±æ–‡æ˜ å°„æ–‡ä»¶: {MAPPING_FILE}")
    print(f"ğŸ“¦ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print("=" * 60)
    print("ğŸ“‹ åŠŸèƒ½è¯´æ˜:")
    print("  - è¯»å–æ ‡æ³¨æ–‡ä»¶ä¸­çš„ä¸­æ–‡æ ‡ç­¾")
    print("  - ä½¿ç”¨ä¸­è‹±æ–‡å¯¹ç…§è¡¨è½¬æ¢ä¸ºè‹±æ–‡æ ‡ç­¾")
    print("  - ç”Ÿæˆé€‚åˆè§†é¢‘VQAæ¨¡å‹è®­ç»ƒçš„æ•°æ®é›†")
    print("  - ä¸è¿›è¡Œè§†é¢‘æŠ½å¸§ï¼Œä½¿ç”¨åŸå§‹è§†é¢‘æ–‡ä»¶")
    print("=" * 60)
    
    # æ£€æŸ¥è¾“å…¥
    if not os.path.exists(ANNOTATIONS_DIR):
        logger.error(f"æ ‡æ³¨ç›®å½•ä¸å­˜åœ¨: {ANNOTATIONS_DIR}")
        print(f"\nâŒ é”™è¯¯: æ ‡æ³¨ç›®å½•ä¸å­˜åœ¨: {ANNOTATIONS_DIR}")
        print("è¯·å…ˆè¿è¡Œæ ‡æ³¨ç”Ÿæˆè„šæœ¬ç”Ÿæˆæ ‡æ³¨æ–‡ä»¶")
        return
    
    if not os.path.exists(MAPPING_FILE):
        logger.warning(f"æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {MAPPING_FILE}")
        logger.warning("å°†ä½¿ç”¨åŸä¸­æ–‡æ ‡ç­¾ä½œä¸ºè‹±æ–‡æ ‡ç­¾")
        print(f"\nâš ï¸  è­¦å‘Š: æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {MAPPING_FILE}")
        print("å°†ä½¿ç”¨åŸä¸­æ–‡æ ‡ç­¾ä½œä¸ºè‹±æ–‡æ ‡ç­¾")
        print("å»ºè®®æä¾›ä¸­è‹±æ–‡å¯¹ç…§è¡¨ä»¥è·å¾—æ›´å¥½çš„æ ‡ç­¾æ˜ å°„")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = AnnotationProcessor(
        annotations_dir=ANNOTATIONS_DIR,
        mapping_file=MAPPING_FILE,
        output_dir=OUTPUT_DIR
    )
    
    # å¤„ç†æ‰€æœ‰æ ‡æ³¨
    start_time = time.time()
    result = processor.process_all_annotations()
    elapsed_time = time.time() - start_time
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ‰ å¤„ç†å®Œæˆ")
    print("=" * 60)
    print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
    print(f"ğŸ“Š å¤„ç†ç»“æœ:")
    print(f"  âœ… æ€»æ ·æœ¬æ•°: {result['total_samples']}")
    
    if result.get('category_stats'):
        print(f"  ğŸ“‚ ç±»åˆ«ç»Ÿè®¡ (å‰10ä¸ª):")
        sorted_categories = sorted(result['category_stats'].items(), 
                                  key=lambda x: x[1], reverse=True)
        for category, count in sorted_categories[:10]:
            print(f"    - {category}: {count}")
        if len(sorted_categories) > 10:
            print(f"    ... è¿˜æœ‰ {len(sorted_categories) - 10} ä¸ªç±»åˆ«")
    
    if result.get('unmapped_labels'):
        print(f"  âš ï¸  æœªæ˜ å°„æ ‡ç­¾: {len(result['unmapped_labels'])} ä¸ª")
        print(f"    æ˜ å°„ç‡: {100 * (1 - len(result['unmapped_labels']) / result['total_samples']):.1f}%")
        print(f"    æœªæ˜ å°„æ ‡ç­¾ç¤ºä¾‹:")
        for label in list(result['unmapped_labels'])[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            print(f"      - {label}")
        if len(result['unmapped_labels']) > 5:
            print(f"      ... è¿˜æœ‰ {len(result['unmapped_labels']) - 5} ä¸ª")
    
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print("ç›®å½•ç»“æ„:")
    print(f"  {OUTPUT_DIR}/")
    print(f"  â””â”€â”€ converted_annotations/    # è½¬æ¢åçš„æ ‡æ³¨")
    print(f"      â”œâ”€â”€ video_vqa_dataset.json     # å®Œæ•´æ•°æ®é›†")
    print(f"      â”œâ”€â”€ simple_dataset.json        # ç®€åŒ–æ•°æ®é›†")
    print(f"      â”œâ”€â”€ existing_videos_dataset.json  # ä»…åŒ…å«å­˜åœ¨è§†é¢‘çš„æ•°æ®é›†")
    print(f"      â”œâ”€â”€ statistics.json            # ç»Ÿè®¡ä¿¡æ¯")
    print(f"      â”œâ”€â”€ label_mapping_stats.json   # æ ‡ç­¾æ˜ å°„ç»Ÿè®¡")
    print(f"      â””â”€â”€ [ç±»åˆ«].json               # æŒ‰ç±»åˆ«åˆ†çš„æ•°æ®")
    
    print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("1. æ£€æŸ¥æœªæ˜ å°„çš„æ ‡ç­¾ï¼Œæ›´æ–°ä¸­è‹±æ–‡å¯¹ç…§è¡¨")
    print("2. æŸ¥çœ‹è§†é¢‘å­˜åœ¨æƒ…å†µï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®")
    print("3. ä½¿ç”¨è½¬æ¢åçš„æ•°æ®é›†è¿›è¡Œè§†é¢‘VQAæ¨¡å‹è®­ç»ƒ")
    print("4. å¦‚æœè§†é¢‘ç¼ºå¤±è¾ƒå¤šï¼Œæ£€æŸ¥è§†é¢‘ä¸‹è½½æ˜¯å¦å®Œæ•´")
    
    print("=" * 60)
    
    # æ˜¾ç¤ºç”Ÿæˆçš„æ ‡æ³¨ç¤ºä¾‹
    simple_file = os.path.join(OUTPUT_DIR, "converted_annotations", "simple_dataset.json")
    if os.path.exists(simple_file):
        with open(simple_file, 'r', encoding='utf-8') as f:
            simple_data = json.load(f)
        
        if simple_data:
            print(f"\nğŸ“ æ ‡æ³¨ç¤ºä¾‹ (å‰2ä¸ª):")
            for i, item in enumerate(simple_data[:2], 1):
                print(f"\n  {i}. ID: {item.get('id', 'N/A')}")
                print(f"     è§†é¢‘è·¯å¾„: {item.get('video_path', 'N/A')[:80]}...")
                print(f"     è§†é¢‘å­˜åœ¨: {item.get('video_exists', False)}")
                print(f"     ä¸­æ–‡æ ‡ç­¾: {item.get('label_zh', 'N/A')}")
                print(f"     è‹±æ–‡æ ‡ç­¾: {item.get('label_en', 'N/A')}")
                print(f"     æ—¶é—´èŒƒå›´: {item.get('time_range', [])}")
                print(f"     æ—¶é•¿: {item.get('duration', 0)}s")
            print("\n" + "=" * 60)

if __name__ == "__main__":
    main()