import os
import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from datetime import datetime
import time
import traceback
from tqdm import tqdm
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/root/workspace/sliced_label_conversion.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SlicedLabelConverter:
    """åˆ‡ç‰‡æ ‡ç­¾è½¬æ¢å™¨ï¼Œå¤„ç†ä¸­è‹±æ–‡æ ‡ç­¾æ˜ å°„ï¼ˆä»…ç²¾ç¡®åŒ¹é…ï¼‰"""
    
    def __init__(self, mapping_file: str):
        self.mapping_file = mapping_file
        self.chinese_to_english = {}
        self.english_to_chinese = {}
        self.unmapped_labels = set()
        self.load_mapping()
    
    def load_mapping(self):
        """åŠ è½½ä¸­è‹±æ–‡å¯¹ç…§è¡¨ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰"""
        try:
            if not os.path.exists(self.mapping_file):
                logger.warning(f"ä¸­è‹±æ–‡å¯¹ç…§è¡¨ä¸å­˜åœ¨: {self.mapping_file}")
                return
            
            df = pd.read_excel(self.mapping_file)
            logger.info(f"åŠ è½½ä¸­è‹±æ–‡å¯¹ç…§è¡¨: {len(df)} è¡Œ")
            
            # ç¡®ä¿åˆ—åæ­£ç¡®
            required_columns = ['ä¸­æ–‡æ ‡ç­¾', 'è‹±æ–‡æ ‡ç­¾']
            for col in required_columns:
                if col not in df.columns:
                    logger.error(f"å¯¹ç…§è¡¨ç¼ºå°‘å¿…è¦åˆ—: {col}")
                    return
            
            # æ„å»ºæ˜ å°„
            for _, row in df.iterrows():
                chinese = str(row['ä¸­æ–‡æ ‡ç­¾']).strip()
                english = str(row['è‹±æ–‡æ ‡ç­¾']).strip()
                
                if chinese and english and chinese != 'nan' and english != 'nan':
                    self.chinese_to_english[chinese] = english
                    self.english_to_chinese[english] = chinese
                    logger.debug(f"æ˜ å°„: {chinese} -> {english}")
            
            logger.info(f"åŠ è½½äº† {len(self.chinese_to_english)} ä¸ªæ ‡ç­¾æ˜ å°„")
            
        except Exception as e:
            logger.error(f"åŠ è½½æ ‡ç­¾æ˜ å°„å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
    
    def convert_label(self, chinese_label: str) -> Tuple[str, bool]:
        """
        è½¬æ¢ä¸­æ–‡æ ‡ç­¾ä¸ºè‹±æ–‡æ ‡ç­¾
        è¿”å›: (è‹±æ–‡æ ‡ç­¾, æ˜¯å¦æˆåŠŸæ˜ å°„)
        åªè¿›è¡Œç²¾ç¡®åŒ¹é…ï¼Œä¸åŒ¹é…åˆ™è¿”å›åŸä¸­æ–‡æ ‡ç­¾
        """
        if not chinese_label:
            return "", False
        
        # ä»…å°è¯•å®Œå…¨åŒ¹é…
        if chinese_label in self.chinese_to_english:
            return self.chinese_to_english[chinese_label], True
        else:
            # è®°å½•æœªæ˜ å°„çš„æ ‡ç­¾
            self.unmapped_labels.add(chinese_label)
            logger.warning(f"æœªæ‰¾åˆ°æ˜ å°„çš„æ ‡ç­¾: {chinese_label}")
            # è¿”å›åŸä¸­æ–‡æ ‡ç­¾
            return chinese_label, False

class SlicedAnnotationProcessor:
    """åˆ‡ç‰‡æ ‡æ³¨å¤„ç†å™¨ï¼Œè½¬æ¢æ ‡ç­¾å¹¶å‡†å¤‡è§†é¢‘VQAå¾®è°ƒæ•°æ®"""
    
    def __init__(self, annotations_dir: str, mapping_file: str, output_dir: str, 
                 slice_video_dir: str = None):
        self.annotations_dir = annotations_dir
        self.mapping_file = mapping_file
        self.output_dir = output_dir
        self.slice_video_dir = slice_video_dir
        self.label_converter = SlicedLabelConverter(mapping_file)
        
    def process_all_annotations(self) -> Dict:
        """å¤„ç†æ‰€æœ‰åˆ‡ç‰‡æ ‡æ³¨æ–‡ä»¶"""
        # æŸ¥æ‰¾æ‰€æœ‰æ ‡æ³¨æ–‡ä»¶
        annotation_files = []
        for file in os.listdir(self.annotations_dir):
            if file.endswith('.json') and file not in ['summary.json', 'all_sliced_annotations.json']:
                annotation_files.append(os.path.join(self.annotations_dir, file))
        
        logger.info(f"æ‰¾åˆ° {len(annotation_files)} ä¸ªåˆ‡ç‰‡æ ‡æ³¨æ–‡ä»¶")
        
        all_converted_data = []
        category_stats = {}
        
        for annotation_file in tqdm(annotation_files, desc="å¤„ç†æ ‡æ³¨æ–‡ä»¶"):
            category_name = os.path.basename(annotation_file).replace('.json', '')
            category_data = self.process_single_file(annotation_file, category_name)
            
            if category_data:
                category_stats[category_name] = len(category_data)
                all_converted_data.extend(category_data)
        
        # ä¿å­˜ç»“æœ
        self.save_results(all_converted_data, category_stats)
        
        return {
            "total_samples": len(all_converted_data),
            "category_stats": category_stats,
            "unmapped_labels": list(self.label_converter.unmapped_labels)
        }
    
    def process_single_file(self, annotation_file: str, category_name: str) -> List[Dict]:
        """å¤„ç†å•ä¸ªåˆ‡ç‰‡æ ‡æ³¨æ–‡ä»¶"""
        try:
            with open(annotation_file, 'r', encoding='utf-8') as f:
                annotations = json.load(f)
            
            converted_annotations = []
            
            for ann in tqdm(annotations, desc=f"å¤„ç† {category_name}", leave=False):
                converted = self.process_single_sliced_annotation(ann)
                if converted:
                    converted_annotations.append(converted)
            
            return converted_annotations
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {annotation_file}: {str(e)}")
            logger.error(traceback.format_exc())
            return []
    
    def process_single_sliced_annotation(self, annotation: Dict) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªåˆ‡ç‰‡æ ‡æ³¨"""
        try:
            chinese_label = annotation.get('label', '')
            
            if not chinese_label:
                logger.warning(f"è·³è¿‡æ— æ•ˆæ ‡æ³¨: {annotation.get('id', 'unknown')}")
                return None
            
            # è½¬æ¢æ ‡ç­¾
            english_label, mapped = self.label_converter.convert_label(chinese_label)
            
            # è·å–åˆ‡ç‰‡è§†é¢‘è·¯å¾„ - å…³é”®ä¿®æ”¹ç‚¹ï¼
            # åˆ‡ç‰‡æ ‡æ³¨ä½¿ç”¨'slice_video_path'å­—æ®µï¼Œè€Œä¸æ˜¯'original_video'
            video_path = annotation.get('slice_video_path', '')
            slice_key = annotation.get('slice_key', '')
            
            # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•ä»slice_keyæ„é€ 
            if not video_path or not os.path.exists(video_path):
                if slice_key:
                    # å°è¯•ä»slice_keyæ„é€ è·¯å¾„
                    video_path = self._slice_key_to_video_path(slice_key)
                else:
                    video_path = ''
            
            # éªŒè¯è§†é¢‘æ˜¯å¦å­˜åœ¨
            video_exists = os.path.exists(video_path) if video_path else False
            
            # å¦‚æœè§†é¢‘ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾
            if not video_exists and video_path:
                # å°è¯•åœ¨åˆ‡ç‰‡è§†é¢‘ç›®å½•ä¸­æŸ¥æ‰¾
                if self.slice_video_dir and os.path.exists(self.slice_video_dir):
                    # ä»å®Œæ•´è·¯å¾„ä¸­æå–æ–‡ä»¶å
                    filename = os.path.basename(video_path)
                    if filename:
                        # åœ¨åˆ‡ç‰‡ç›®å½•ä¸­é€’å½’æŸ¥æ‰¾æ–‡ä»¶
                        found_path = self._find_video_in_slice_dir(filename)
                        if found_path:
                            video_path = found_path
                            video_exists = True
                            logger.info(f"æ‰¾åˆ°åˆ‡ç‰‡è§†é¢‘: {filename} -> {found_path}")
            
            if not video_exists and video_path:
                logger.warning(f"åˆ‡ç‰‡è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            
            # å‡†å¤‡è¾“å‡ºæ•°æ® - é€‚åº”åˆ‡ç‰‡è§†é¢‘ç»“æ„
            result = {
                "id": annotation.get('id', ''),
                "slice_key": slice_key,
                "video_path": video_path,
                "video_exists": video_exists,
                "label_zh": chinese_label,
                "label_en": english_label,
                "label_mapped": mapped,
                "time_range_in_slice": annotation.get('time_range_in_slice', []),  # åœ¨åˆ‡ç‰‡ä¸­çš„æ—¶é—´
                "slice_window": annotation.get('slice_window', []),  # åˆ‡ç‰‡åœ¨åŸå§‹è§†é¢‘ä¸­çš„çª—å£
                "duration_in_slice": annotation.get('duration_in_slice', 0),  # åœ¨åˆ‡ç‰‡ä¸­çš„æ—¶é•¿
                "original_info": {
                    "clip_path": annotation.get('clip_path', ''),
                    "original_bos_path": annotation.get('original_bos_path', ''),
                    "time_range_original": annotation.get('time_range_original', []),  # åŸå§‹è§†é¢‘ä¸­çš„æ—¶é—´
                    "source_row": annotation.get('source_row', 0)
                },
                "metadata": {
                    "slice_filename": annotation.get('slice_filename', ''),
                    "file_size": annotation.get('file_size', 0)
                }
            }
            
            # æ·»åŠ è§†é¢‘æ–‡ä»¶ä¿¡æ¯
            if video_exists:
                try:
                    file_size = os.path.getsize(video_path)
                    result["metadata"]["video_size"] = file_size
                    result["metadata"]["video_size_mb"] = file_size / (1024 * 1024)
                except:
                    result["metadata"]["video_size"] = 0
                    result["metadata"]["video_size_mb"] = 0
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†åˆ‡ç‰‡æ ‡æ³¨å¤±è´¥ {annotation.get('id', 'unknown')}: {str(e)}")
            logger.error(traceback.format_exc())
            return None
    
    def _slice_key_to_video_path(self, slice_key: str) -> str:
        """ä»slice_keyæ„é€ åˆ‡ç‰‡è§†é¢‘è·¯å¾„"""
        try:
            # slice_keyæ ¼å¼: bos:/neolix-raw/.../video_0_20
            # æå–æ—¶é—´ä¿¡æ¯
            time_match = re.search(r"_(\d+)_(\d+)$", slice_key)
            if not time_match:
                return ""
            
            seg_start, seg_end = time_match.groups()
            
            # æ„é€ åˆ‡ç‰‡æ–‡ä»¶å
            slice_filename = f"slice_{seg_start}_{seg_end}.mp4"
            
            # ä»slice_keyæå–åŸå§‹è·¯å¾„éƒ¨åˆ†
            # ç§»é™¤æ—¶é—´åç¼€
            base_key = re.sub(r"_\d+_\d+$", "", slice_key)
            
            # ç§»é™¤å¼€å¤´çš„bos:å‰ç¼€
            if base_key.startswith("bos:"):
                base_key = base_key[4:]
            
            # ç§»é™¤å¼€å¤´çš„æ–œæ 
            base_key = base_key.lstrip('/')
            
            # å»æ‰'neolix-raw/'å‰ç¼€
            if base_key.startswith("neolix-raw/"):
                base_key = base_key[len("neolix-raw/"):]
            
            # ç§»é™¤æœ«å°¾çš„video
            if base_key.endswith("video"):
                base_key = base_key[:-5]
            
            # æ„å»ºå®Œæ•´è·¯å¾„
            if self.slice_video_dir:
                # æ„é€ ç±»ä¼¼: sliced_videos/raw_clips/.../slices/slice_0_20.mp4
                video_path = os.path.join(
                    self.slice_video_dir,
                    base_key.rstrip('/'),
                    "slices",
                    slice_filename
                )
            else:
                # é»˜è®¤è·¯å¾„
                video_path = os.path.join(
                    "/root/workspace/downloaded_videos_2fps/sliced_videos",
                    base_key.rstrip('/'),
                    "slices",
                    slice_filename
                )
            
            return video_path
            
        except Exception as e:
            logger.error(f"ä»slice_keyæ„é€ è·¯å¾„å¤±è´¥ {slice_key}: {str(e)}")
            return ""
    
    def _find_video_in_slice_dir(self, filename: str) -> Optional[str]:
        """åœ¨åˆ‡ç‰‡è§†é¢‘ç›®å½•ä¸­æŸ¥æ‰¾æ–‡ä»¶"""
        if not self.slice_video_dir or not os.path.exists(self.slice_video_dir):
            return None
        
        for root, dirs, files in os.walk(self.slice_video_dir):
            if filename in files:
                return os.path.join(root, filename)
        
        return None
    
    def save_results(self, all_data: List[Dict], category_stats: Dict):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_annotations_dir = os.path.join(self.output_dir, "converted_sliced_annotations")
        os.makedirs(output_annotations_dir, exist_ok=True)
        
        # ç»Ÿè®¡è§†é¢‘å­˜åœ¨æƒ…å†µ
        video_exists_count = sum(1 for item in all_data if item.get("video_exists", False))
        video_missing_count = len(all_data) - video_exists_count
        
        # ç»Ÿè®¡æ˜ å°„æƒ…å†µ
        mapped_count = sum(1 for item in all_data if item.get("label_mapped", False))
        unmapped_count = len(all_data) - mapped_count
        
        # 1. ä¿å­˜å®Œæ•´æ•°æ®é›†
        output_file = os.path.join(output_annotations_dir, "sliced_video_vqa_dataset.json")
        
        dataset = {
            "version": "1.0.0",
            "description": "Sliced Video VQA dataset with English labels (20-second clips)",
            "created": datetime.now().isoformat(),
            "source_annotations": self.annotations_dir,
            "slice_video_dir": self.slice_video_dir,
            "statistics": {
                "total_samples": len(all_data),
                "video_exists": video_exists_count,
                "video_missing": video_missing_count,
                "labels_mapped": mapped_count,
                "labels_unmapped": unmapped_count,
                "mapping_rate": mapped_count / len(all_data) if len(all_data) > 0 else 0,
                "categories": category_stats,
                "unmapped_labels": list(self.label_converter.unmapped_labels)
            },
            "data": all_data
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜å®Œæ•´åˆ‡ç‰‡æ•°æ®é›†: {output_file} ({len(all_data)} ä¸ªæ ·æœ¬)")
        logger.info(f"è§†é¢‘å­˜åœ¨: {video_exists_count}, è§†é¢‘ç¼ºå¤±: {video_missing_count}")
        logger.info(f"æ ‡ç­¾æ˜ å°„: {mapped_count}, æœªæ˜ å°„: {unmapped_count}")
        
        # 2. æŒ‰ç±»åˆ«ä¿å­˜
        categories_data = {}
        for item in all_data:
            label_zh = item.get("label_zh", "unknown")
            if label_zh not in categories_data:
                categories_data[label_zh] = []
            categories_data[label_zh].append(item)
        
        for label_zh, items in categories_data.items():
            # åˆ›å»ºå®‰å…¨æ–‡ä»¶å
            safe_name = self._create_safe_filename(label_zh)
            category_file = os.path.join(
                output_annotations_dir, f"{safe_name}.json"
            )
            
            # ç»Ÿè®¡è¯¥ç±»åˆ«çš„è§†é¢‘å­˜åœ¨æƒ…å†µ
            category_video_exists = sum(1 for item in items if item.get("video_exists", False))
            
            category_dataset = {
                "label_zh": label_zh,
                "label_en": items[0].get("label_en", "") if items else "",
                "count": len(items),
                "video_exists": category_video_exists,
                "video_missing": len(items) - category_video_exists,
                "data": items
            }
            
            with open(category_file, 'w', encoding='utf-8') as f:
                json.dump(category_dataset, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜äº† {len(categories_data)} ä¸ªç±»åˆ«æ–‡ä»¶")
        
        # 3. ä¿å­˜ç®€åŒ–ç‰ˆæœ¬ï¼ˆç”¨äºåç»­å¤„ç†ï¼‰
        simple_data = []
        for item in all_data:
            simple_item = {
                "id": item.get("id", ""),
                "slice_key": item.get("slice_key", ""),
                "video_path": item.get("video_path", ""),
                "video_exists": item.get("video_exists", False),
                "label_zh": item.get("label_zh", ""),
                "label_en": item.get("label_en", ""),
                "time_range_in_slice": item.get("time_range_in_slice", []),
                "slice_window": item.get("slice_window", []),
                "duration_in_slice": item.get("duration_in_slice", 0)
            }
            simple_data.append(simple_item)
        
        simple_file = os.path.join(output_annotations_dir, "simple_sliced_dataset.json")
        with open(simple_file, 'w', encoding='utf-8') as f:
            json.dump(simple_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜ç®€åŒ–åˆ‡ç‰‡æ•°æ®é›†: {simple_file}")
        
        # 4. ä¿å­˜ä»…åŒ…å«è§†é¢‘å­˜åœ¨çš„æ ·æœ¬
        existing_videos_data = [item for item in all_data if item.get("video_exists", False)]
        existing_file = os.path.join(output_annotations_dir, "existing_sliced_videos_dataset.json")
        
        existing_dataset = {
            "version": "1.0.0",
            "description": "Sliced Video VQA dataset with existing videos only",
            "created": datetime.now().isoformat(),
            "statistics": {
                "total_samples": len(existing_videos_data),
                "categories": {k: v for k, v in category_stats.items() 
                             if k in {item.get("label_zh") for item in existing_videos_data}}
            },
            "data": existing_videos_data
        }
        
        with open(existing_file, 'w', encoding='utf-8') as f:
            json.dump(existing_dataset, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜ä»…åŒ…å«å­˜åœ¨è§†é¢‘çš„åˆ‡ç‰‡æ•°æ®é›†: {existing_file} ({len(existing_videos_data)} ä¸ªæ ·æœ¬)")
        
        # 5. ä¿å­˜CSVæ ¼å¼ï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
        csv_data = []
        for item in all_data:
            csv_item = {
                "id": item.get("id", ""),
                "slice_key": item.get("slice_key", ""),
                "video_path": item.get("video_path", ""),
                "video_exists": item.get("video_exists", False),
                "label_zh": item.get("label_zh", ""),
                "label_en": item.get("label_en", ""),
                "time_start": item.get("time_range_in_slice", [])[0] if item.get("time_range_in_slice") else 0,
                "time_end": item.get("time_range_in_slice", [])[1] if item.get("time_range_in_slice") else 0,
                "slice_start": item.get("slice_window", [])[0] if item.get("slice_window") else 0,
                "slice_end": item.get("slice_window", [])[1] if item.get("slice_window") else 0,
                "duration": item.get("duration_in_slice", 0)
            }
            csv_data.append(csv_item)
        
        csv_file = os.path.join(output_annotations_dir, "sliced_dataset.csv")
        df = pd.DataFrame(csv_data)
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        logger.info(f"ä¿å­˜CSVæ ¼å¼æ•°æ®é›†: {csv_file}")
        
        # 6. ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "processing_time": datetime.now().isoformat(),
            "total_samples": len(all_data),
            "video_exists": video_exists_count,
            "video_missing": video_missing_count,
            "labels_mapped": mapped_count,
            "labels_unmapped": unmapped_count,
            "mapping_rate": mapped_count / len(all_data) if len(all_data) > 0 else 0,
            "categories": category_stats,
            "unmapped_labels": list(self.label_converter.unmapped_labels),
            "unmapped_count": len(self.label_converter.unmapped_labels)
        }
        
        stats_file = os.path.join(output_annotations_dir, "sliced_statistics.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜åˆ‡ç‰‡ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
        
        # 7. ä¿å­˜æ ‡ç­¾æ˜ å°„ç»Ÿè®¡
        label_mapping_stats = []
        for item in all_data:
            label_mapping_stats.append({
                "id": item.get("id", ""),
                "slice_key": item.get("slice_key", ""),
                "label_zh": item.get("label_zh", ""),
                "label_en": item.get("label_en", ""),
                "mapped": item.get("label_mapped", False)
            })
        
        mapping_stats_file = os.path.join(output_annotations_dir, "sliced_label_mapping_stats.json")
        with open(mapping_stats_file, 'w', encoding='utf-8') as f:
            json.dump(label_mapping_stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜åˆ‡ç‰‡æ ‡ç­¾æ˜ å°„ç»Ÿè®¡: {mapping_stats_file}")
    
    def _create_safe_filename(self, text: str) -> str:
        """åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å"""
        # æ›¿æ¢éæ³•å­—ç¬¦
        safe = re.sub(r'[<>:"/\\|?*]', '_', text)
        safe = re.sub(r'\s+', '_', safe)
        safe = safe.strip('._')
        
        # é™åˆ¶é•¿åº¦
        if len(safe) > 100:
            safe = safe[:100]
        
        return safe

def verify_sliced_annotations(annotations_dir: str):
    """éªŒè¯åˆ‡ç‰‡æ ‡æ³¨æ–‡ä»¶"""
    print("\n" + "=" * 60)
    print("éªŒè¯åˆ‡ç‰‡æ ‡æ³¨æ–‡ä»¶...")
    print("=" * 60)
    
    if not os.path.exists(annotations_dir):
        print(f"âŒ åˆ‡ç‰‡æ ‡æ³¨ç›®å½•ä¸å­˜åœ¨: {annotations_dir}")
        return 0
    
    # æŸ¥æ‰¾æ‰€æœ‰æ ‡æ³¨æ–‡ä»¶
    annotation_files = []
    for file in os.listdir(annotations_dir):
        if file.endswith('.json'):
            annotation_files.append(file)
    
    print(f"æ‰¾åˆ° {len(annotation_files)} ä¸ªæ ‡æ³¨æ–‡ä»¶")
    
    if annotation_files:
        print("\næ ‡æ³¨æ–‡ä»¶åˆ—è¡¨:")
        for i, file in enumerate(annotation_files[:10], 1):
            file_path = os.path.join(annotations_dir, file)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                if isinstance(data, list):
                    count = len(data)
                else:
                    count = len(data.get('data', [])) if isinstance(data, dict) else 0
                print(f"  {i}. {file} ({count} ä¸ªæ ‡æ³¨)")
            except:
                print(f"  {i}. {file} (è¯»å–å¤±è´¥)")
        
        if len(annotation_files) > 10:
            print(f"  ... è¿˜æœ‰ {len(annotation_files) - 10} ä¸ªæ–‡ä»¶")
    
    return len(annotation_files)

def main():
    """ä¸»å‡½æ•°"""
    # è¾“å…¥è·¯å¾„
    ANNOTATIONS_DIR = "/root/workspace/sliced_vqa_annotations/sliced_annotations"
    MAPPING_FILE = "/root/workspace/LLaMA-Factory/data/ä¸­è‹±å¯¹ç…§è¡¨_12tags.xlsx"
    SLICE_VIDEO_DIR = "/root/workspace/downloaded_videos_2fps/sliced_videos"
    OUTPUT_DIR = "/root/workspace/sliced_vqa_dataset_prepared"
    
    print("=" * 60)
    print("ğŸ¯ åˆ‡ç‰‡è§†é¢‘VQAæ ‡æ³¨æ ‡ç­¾è½¬æ¢å·¥å…·")
    print("=" * 60)
    print(f"ğŸ“ åˆ‡ç‰‡æ ‡æ³¨ç›®å½•: {ANNOTATIONS_DIR}")
    print(f"ğŸ“ åˆ‡ç‰‡è§†é¢‘ç›®å½•: {SLICE_VIDEO_DIR}")
    print(f"ğŸ“„ ä¸­è‹±æ–‡æ˜ å°„æ–‡ä»¶: {MAPPING_FILE}")
    print(f"ğŸ“¦ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print("=" * 60)
    print("ğŸ“‹ åŠŸèƒ½è¯´æ˜:")
    print("  - è¯»å–åˆ‡ç‰‡æ ‡æ³¨æ–‡ä»¶ï¼ˆ20ç§’è§†é¢‘åˆ‡ç‰‡ï¼‰")
    print("  - ä½¿ç”¨ä¸­è‹±æ–‡å¯¹ç…§è¡¨è½¬æ¢ä¸ºè‹±æ–‡æ ‡ç­¾")
    print("  - ç”Ÿæˆé€‚åˆè§†é¢‘VQAæ¨¡å‹è®­ç»ƒçš„åˆ‡ç‰‡æ•°æ®é›†")
    print("  - é€‚é…åˆ‡ç‰‡è§†é¢‘çš„ç‰¹æ®Šæ•°æ®ç»“æ„")
    print("=" * 60)
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    annotation_count = verify_sliced_annotations(ANNOTATIONS_DIR)
    if annotation_count == 0:
        logger.error(f"æ²¡æœ‰æ‰¾åˆ°åˆ‡ç‰‡æ ‡æ³¨æ–‡ä»¶: {ANNOTATIONS_DIR}")
        print(f"\nâŒ é”™è¯¯: åˆ‡ç‰‡æ ‡æ³¨ç›®å½•ä¸å­˜åœ¨æˆ–ä¸ºç©º: {ANNOTATIONS_DIR}")
        print("è¯·å…ˆè¿è¡Œåˆ‡ç‰‡æ ‡æ³¨ç”Ÿæˆè„šæœ¬ç”Ÿæˆæ ‡æ³¨æ–‡ä»¶")
        return
    
    if not os.path.exists(SLICE_VIDEO_DIR):
        logger.warning(f"åˆ‡ç‰‡è§†é¢‘ç›®å½•ä¸å­˜åœ¨: {SLICE_VIDEO_DIR}")
        print(f"\nâš ï¸  è­¦å‘Š: åˆ‡ç‰‡è§†é¢‘ç›®å½•ä¸å­˜åœ¨: {SLICE_VIDEO_DIR}")
        print("è§†é¢‘è·¯å¾„éªŒè¯å¯èƒ½ä¼šå¤±è´¥")
    
    if not os.path.exists(MAPPING_FILE):
        logger.warning(f"æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {MAPPING_FILE}")
        logger.warning("å°†ä½¿ç”¨åŸä¸­æ–‡æ ‡ç­¾ä½œä¸ºè‹±æ–‡æ ‡ç­¾")
        print(f"\nâš ï¸  è­¦å‘Š: æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {MAPPING_FILE}")
        print("å°†ä½¿ç”¨åŸä¸­æ–‡æ ‡ç­¾ä½œä¸ºè‹±æ–‡æ ‡ç­¾")
        print("å»ºè®®æä¾›ä¸­è‹±æ–‡å¯¹ç…§è¡¨ä»¥è·å¾—æ›´å¥½çš„æ ‡ç­¾æ˜ å°„")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = SlicedAnnotationProcessor(
        annotations_dir=ANNOTATIONS_DIR,
        mapping_file=MAPPING_FILE,
        output_dir=OUTPUT_DIR,
        slice_video_dir=SLICE_VIDEO_DIR
    )
    
    # å¤„ç†æ‰€æœ‰æ ‡æ³¨
    start_time = time.time()
    result = processor.process_all_annotations()
    elapsed_time = time.time() - start_time
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ‰ åˆ‡ç‰‡æ ‡æ³¨è½¬æ¢å®Œæˆ")
    print("=" * 60)
    print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
    print(f"ğŸ“Š å¤„ç†ç»“æœ:")
    print(f"  âœ… æ€»æ ·æœ¬æ•°: {result['total_samples']}")
    
    if result.get('category_stats'):
        print(f"  ğŸ“‚ ç±»åˆ«ç»Ÿè®¡ (å‰10ä¸ª):")
        sorted_categories = sorted(result['category_stats'].items(), 
                                  key=lambda x: x[1], reverse=True)
        for category, count in sorted_categories[:10]:
            print(f"    - {category}: {count}")
        if len(sorted_categories) > 10:
            print(f"    ... è¿˜æœ‰ {len(sorted_categories) - 10} ä¸ªç±»åˆ«")
    
    if result.get('unmapped_labels'):
        print(f"  âš ï¸  æœªæ˜ å°„æ ‡ç­¾: {len(result['unmapped_labels'])} ä¸ª")
        mapped_count = result['total_samples'] - len(result['unmapped_labels'])
        mapping_rate = mapped_count / result['total_samples'] if result['total_samples'] > 0 else 0
        print(f"    æ˜ å°„ç‡: {mapping_rate*100:.1f}% ({mapped_count}/{result['total_samples']})")
        print(f"    æœªæ˜ å°„æ ‡ç­¾ç¤ºä¾‹:")
        for label in list(result['unmapped_labels'])[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            print(f"      - {label}")
        if len(result['unmapped_labels']) > 5:
            print(f"      ... è¿˜æœ‰ {len(result['unmapped_labels']) - 5} ä¸ª")
    
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print("ç›®å½•ç»“æ„:")
    print(f"  {OUTPUT_DIR}/")
    print(f"  â””â”€â”€ converted_sliced_annotations/    # è½¬æ¢åçš„åˆ‡ç‰‡æ ‡æ³¨")
    print(f"      â”œâ”€â”€ sliced_video_vqa_dataset.json     # å®Œæ•´åˆ‡ç‰‡æ•°æ®é›†")
    print(f"      â”œâ”€â”€ simple_sliced_dataset.json        # ç®€åŒ–åˆ‡ç‰‡æ•°æ®é›†")
    print(f"      â”œâ”€â”€ existing_sliced_videos_dataset.json  # ä»…åŒ…å«å­˜åœ¨è§†é¢‘çš„åˆ‡ç‰‡æ•°æ®é›†")
    print(f"      â”œâ”€â”€ sliced_dataset.csv               # CSVæ ¼å¼æ•°æ®é›†")
    print(f"      â”œâ”€â”€ sliced_statistics.json           # ç»Ÿè®¡ä¿¡æ¯")
    print(f"      â”œâ”€â”€ sliced_label_mapping_stats.json  # æ ‡ç­¾æ˜ å°„ç»Ÿè®¡")
    print(f"      â””â”€â”€ [ç±»åˆ«].json                     # æŒ‰ç±»åˆ«åˆ†çš„æ•°æ®")
    
    print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("1. æ£€æŸ¥æœªæ˜ å°„çš„æ ‡ç­¾ï¼Œæ›´æ–°ä¸­è‹±æ–‡å¯¹ç…§è¡¨")
    print("2. æŸ¥çœ‹è§†é¢‘å­˜åœ¨æƒ…å†µï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®")
    print("3. å¯¹åˆ‡ç‰‡è§†é¢‘è¿›è¡ŒæŠ½å¸§: python /root/workspace/LLaMA-Factory/scene_tag/1.5_get_frames_squeeze.py -i /root/workspace/downloaded_videos_2fps/sliced_videos")
    print("4. ä½¿ç”¨è½¬æ¢åçš„æ•°æ®é›†è¿›è¡Œè§†é¢‘VQAæ¨¡å‹è®­ç»ƒ")
    
    # æ˜¾ç¤ºç”Ÿæˆçš„æ ‡æ³¨ç¤ºä¾‹
    simple_file = os.path.join(OUTPUT_DIR, "converted_sliced_annotations", "simple_sliced_dataset.json")
    if os.path.exists(simple_file):
        with open(simple_file, 'r', encoding='utf-8') as f:
            simple_data = json.load(f)
        
        if simple_data:
            print(f"\nğŸ“ åˆ‡ç‰‡æ ‡æ³¨ç¤ºä¾‹ (å‰2ä¸ª):")
            for i, item in enumerate(simple_data[:2], 1):
                print(f"\n  {i}. ID: {item.get('id', 'N/A')}")
                print(f"     slice_key: {item.get('slice_key', 'N/A')[:50]}...")
                print(f"     è§†é¢‘è·¯å¾„: {item.get('video_path', 'N/A')[:80]}...")
                print(f"     è§†é¢‘å­˜åœ¨: {item.get('video_exists', False)}")
                print(f"     ä¸­æ–‡æ ‡ç­¾: {item.get('label_zh', 'N/A')}")
                print(f"     è‹±æ–‡æ ‡ç­¾: {item.get('label_en', 'N/A')}")
                print(f"     æ—¶é—´èŒƒå›´(åˆ‡ç‰‡ä¸­): {item.get('time_range_in_slice', [])}")
                print(f"     åˆ‡ç‰‡çª—å£(åŸå§‹è§†é¢‘): {item.get('slice_window', [])}")
                print(f"     æ—¶é•¿(åˆ‡ç‰‡ä¸­): {item.get('duration_in_slice', 0)}s")
            print("\n" + "=" * 60)
    
    # è¾“å‡ºä½¿ç”¨è¯´æ˜
    print(f"\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("1. æ•°æ®é›†å·²å‡†å¤‡å¥½ï¼Œå¯ä»¥ç›´æ¥ç”¨äºè®­ç»ƒ")
    print("2. æ¯ä¸ªæ ·æœ¬åŒ…å«20ç§’åˆ‡ç‰‡è§†é¢‘çš„è·¯å¾„å’Œæ ‡æ³¨")
    print("3. ä½¿ç”¨'slice_key'å¯ä»¥å”¯ä¸€æ ‡è¯†æ¯ä¸ªæ ·æœ¬")
    print("4. ç¡®ä¿åˆ‡ç‰‡è§†é¢‘å·²ä¸‹è½½å¹¶å­˜å‚¨åœ¨æ­£ç¡®ä½ç½®")
    print("5. è®­ç»ƒå‰éœ€è¦å…ˆå¯¹åˆ‡ç‰‡è§†é¢‘è¿›è¡ŒæŠ½å¸§")
    print("=" * 60)

if __name__ == "__main__":
    main()

'''
ç¡®è®¤å‚æ•°ï¼š
    ANNOTATIONS_DIR = "/root/workspace/sliced_vqa_annotations/sliced_annotations"
    SLICE_VIDEO_DIR = "/root/workspace/downloaded_videos_for_segment/sliced_videos"
    
    OUTPUT_DIR = "/root/workspace/sliced_vqa_dataset_prepared"
    MAPPING_FILEæ˜¯ä¸­è‹±æ–‡å¯¹ç…§è¡¨çš„è·¯å¾„
è¿è¡Œå³å¯
'''