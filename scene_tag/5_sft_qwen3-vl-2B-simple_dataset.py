#!/usr/bin/env python3
"""
Qwen3-VL-2B è§†é¢‘VQAå¾®è°ƒè„šæœ¬
ä¿®å¤å¯¹è¯æ ¼å¼é—®é¢˜ï¼Œä¸æ¨ç†è„šæœ¬å®Œå…¨ä¸€è‡´
"""

import os
import json
import torch
import cv2
import base64
import numpy as np
from PIL import Image
from io import BytesIO
from transformers import AutoModelForImageTextToText, AutoProcessor, TrainingArguments, Trainer
import logging
from datetime import datetime
from tqdm import tqdm
import random
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from torch.utils.data import Dataset
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def extract_frames_from_video(video_path: str, num_frames: int = 60) -> List[str]:
    """
    ä»è§†é¢‘ä¸­æå–å¸§ï¼ˆæ¯ç§’1å¸§ï¼Œæœ€å¤š60å¸§ï¼‰
    è¿”å›base64ç¼–ç çš„å›¾ç‰‡åˆ—è¡¨ï¼Œä¸æ¨ç†è„šæœ¬å®Œå…¨ä¸€è‡´
    """
    if not os.path.exists(video_path):
        logger.error(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return []
    
    try:
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            logger.error(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            return []
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        
        if fps <= 0:
            fps = 30
        
        # è®¡ç®—è¦æå–çš„å¸§ç´¢å¼•ï¼ˆæ¯ç§’1å¸§ï¼‰
        frames_to_extract = []
        for i in range(min(num_frames, total_frames // fps)):
            frame_idx = i * fps
            if frame_idx < total_frames:
                frames_to_extract.append(frame_idx)
        
        # æå–å¸§
        frames_base64 = []
        for frame_idx in frames_to_extract:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            
            if not ret:
                blank_frame = np.zeros((480, 640, 3), dtype=np.uint8)
                frame = blank_frame
            else:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # è°ƒæ•´å¤§å°
            h, w = frame.shape[:2]
            if h > 360 or w > 640:
                frame = cv2.resize(frame, (640, 360))
            
            pil_image = Image.fromarray(frame)
            buffered = BytesIO()
            pil_image.save(buffered, format="JPEG", quality=85)
            img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
            frames_base64.append(img_base64)
        
        cap.release()
        
        # ç”¨ç©ºç™½å¸§è¡¥é½
        while len(frames_base64) < num_frames:
            blank_frame = np.zeros((360, 640, 3), dtype=np.uint8)
            pil_image = Image.fromarray(blank_frame.astype(np.uint8))
            buffered = BytesIO()
            pil_image.save(buffered, format="JPEG", quality=85)
            img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
            frames_base64.append(img_base64)
        
        return frames_base64[:num_frames]
        
    except Exception as e:
        logger.error(f"æå–è§†é¢‘å¸§å¤±è´¥ {video_path}: {str(e)}")
        return []

def build_vqa_prompt(question: str, video_duration: int = 60) -> str:
    """
    æ„å»ºè§†é¢‘VQAçš„prompt
    ä¸æ¨ç†è„šæœ¬å®Œå…¨ä¸€è‡´
    """
    prompt = f"""You are watching a {video_duration}-second video of driving scenarios. The video is sampled at 1 frame per second, showing {video_duration} consecutive seconds.

Question: {question}

Please analyze the ego vehicle's behavior and provide the following information:
1. Identify the driving maneuver(s) performed by the ego vehicle
2. Specify the start time and end time for each action (in seconds)
3. Use the format: <driving_maneuver>action_label</driving_maneuver> from <start_time>start_time_value</start_time> to <end_time>end_time_value</end_time> seconds.

If there are multiple actions, list them in chronological order separated by " and ".

Answer:"""
    
    return prompt

def prepare_conversation_format_inference(images_base64: List[str], prompt: str) -> List[Dict]:
    """
    å‡†å¤‡ç¬¦åˆQwen-VLå¯¹è¯æ ¼å¼çš„æ•°æ®ï¼ˆä»…ç”¨æˆ·è¾“å…¥ï¼‰
    ä¸æ¨ç†è„šæœ¬çš„prepare_conversation_formatå‡½æ•°å®Œå…¨ä¸€è‡´
    """
    user_content = []
    
    for img_base64 in images_base64:
        user_content.append({
            "type": "image",
            "image": img_base64
        })
    
    user_content.append({
        "type": "text",
        "text": prompt
    })
    
    conversations = [
        {
            "role": "user",
            "content": user_content
        }
    ]
    
    return conversations

def prepare_conversation_format_training(images_base64: List[str], prompt: str, answer: str) -> List[Dict]:
    """
    å‡†å¤‡è®­ç»ƒç”¨çš„å¯¹è¯æ ¼å¼ï¼ˆç”¨æˆ·è¾“å…¥ + åŠ©æ‰‹å›å¤ï¼‰
    è®­ç»ƒæ—¶éœ€è¦åŒ…å«åŠ©æ‰‹å›å¤
    """
    user_content = []
    
    for img_base64 in images_base64:
        user_content.append({
            "type": "image",
            "image": img_base64
        })
    
    user_content.append({
        "type": "text",
        "text": prompt
    })
    
    conversations = [
        {
            "role": "user",
            "content": user_content
        },
        {
            "role": "assistant",
            "content": answer
        }
    ]
    
    return conversations

class VideoVQADataset(Dataset):
    """è§†é¢‘VQAæ•°æ®é›†ç±»"""
    
    def __init__(self, data_path: str, processor, max_samples: int = None, num_frames: int = 8):
        self.processor = processor
        self.num_frames = num_frames
        self.samples = []
        
        logger.info(f"åŠ è½½æ•°æ®é›†: {data_path}")
        with open(data_path, 'r') as f:
            data = json.load(f)
        
        if isinstance(data, dict) and "data" in data:
            raw_samples = data["data"]
        else:
            raw_samples = data
        
        if max_samples and len(raw_samples) > max_samples:
            raw_samples = random.sample(raw_samples, max_samples)
        
        # å¤„ç†æ ·æœ¬
        for i, sample in enumerate(tqdm(raw_samples, desc="å¤„ç†æ ·æœ¬")):
            try:
                video_path = sample["video_path"]
                question = sample["question"]
                answer = sample["answer"]
                video_duration = sample.get("video_duration", 60)
                
                # æå–å¸§
                images_base64 = extract_frames_from_video(video_path, self.num_frames)
                if len(images_base64) != self.num_frames:
                    logger.warning(f"æ ·æœ¬ {i} å¸§æ•°ä¸æ­£ç¡®: {len(images_base64)} å¸§ï¼ŒæœŸæœ› {self.num_frames} å¸§")
                    continue
                
                # æ„å»ºprompt
                prompt = build_vqa_prompt(question, video_duration)
                
                # æ„å»ºå¯¹è¯ - è®­ç»ƒæ—¶åŒ…å«ç”¨æˆ·å’ŒåŠ©æ‰‹æ¶ˆæ¯
                conversation = prepare_conversation_format_training(images_base64, prompt, answer)
                
                self.samples.append({
                    "conversation": conversation,
                    "video_path": video_path,
                    "question": question,
                    "answer": answer
                })
                
            except Exception as e:
                logger.warning(f"å¤„ç†æ ·æœ¬ {i} å¤±è´¥: {str(e)}")
                continue
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(self.samples)} ä¸ªæ ·æœ¬")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        item = self.samples[idx]
        
        # ä½¿ç”¨å¤„ç†å™¨å¤„ç†å¯¹è¯
        try:
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.processor.apply_chat_template(
                item["conversation"],
                tokenize=False,
                add_generation_prompt=False
            )
            
            # æå–å›¾ç‰‡
            images = []
            for content in item["conversation"][0]["content"]:
                if content["type"] == "image":
                    # è§£ç base64å›¾ç‰‡
                    image_data = base64.b64decode(content["image"])
                    image = Image.open(BytesIO(image_data)).convert("RGB")
                    images.append(image)
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=[text],
                images=[images],
                return_tensors="pt",
                padding=True
            )
            
            # å¤„ç†æ ‡ç­¾
            with self.processor.tokenizer.as_target_tokenizer():
                labels = self.processor.tokenizer(
                    item["answer"],
                    padding=True,
                    return_tensors="pt",
                    max_length=512
                )
            
            return {
                "input_ids": inputs["input_ids"].squeeze(0),
                "attention_mask": inputs["attention_mask"].squeeze(0),
                "pixel_values": inputs["pixel_values"].squeeze(0),
                "labels": labels["input_ids"].squeeze(0)
            }
            
        except Exception as e:
            logger.error(f"å¤„ç†æ ·æœ¬ {idx} å¤±è´¥: {str(e)}")
            # è¿”å›ä¸€ä¸ªç©ºæ ·æœ¬
            return {
                "input_ids": torch.zeros(1, 10, dtype=torch.long),
                "attention_mask": torch.zeros(1, 10, dtype=torch.long),
                "pixel_values": torch.zeros(1, 3, 360, 640),
                "labels": torch.zeros(1, 10, dtype=torch.long)
            }

@dataclass
class DataCollatorForVideoVQA:
    """æ•°æ®æ”¶é›†å™¨"""
    processor: Any
    
    def __call__(self, features: List[Dict]) -> Dict[str, Any]:
        batch = {}
        
        # åˆ†ç¦»è¾“å…¥ç‰¹å¾
        input_ids = [f["input_ids"] for f in features]
        attention_masks = [f["attention_mask"] for f in features]
        pixel_values = [f["pixel_values"] for f in features]
        labels = [f["labels"] for f in features]
        
        # å¡«å……input_idså’Œattention_mask
        batch_input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids, batch_first=True, padding_value=self.processor.tokenizer.pad_token_id
        )
        batch_attention_mask = torch.nn.utils.rnn.pad_sequence(
            attention_masks, batch_first=True, padding_value=0
        )
        
        # å¡«å……labels
        batch_labels = torch.nn.utils.rnn.pad_sequence(
            labels, batch_first=True, padding_value=-100
        )
        
        # å †å pixel_values
        batch_pixel_values = torch.stack(pixel_values)
        
        return {
            "input_ids": batch_input_ids,
            "attention_mask": batch_attention_mask,
            "pixel_values": batch_pixel_values,
            "labels": batch_labels
        }

def train_model():
    """è®­ç»ƒæ¨¡å‹ä¸»å‡½æ•°"""
    print("=" * 60)
    print("Qwen3-VL-2B è§†é¢‘VQAè®­ç»ƒ (ä¿®å¤å¯¹è¯æ ¼å¼ç‰ˆ)")
    print("=" * 60)
    
    # é…ç½®
    MODEL_NAME = "Qwen/Qwen3-VL-2B-Instruct"
    TRAIN_DATA = "/root/workspace/video_vqa_dataset/video_vqa_dataset_20251231_162049/train.json"
    OUTPUT_DIR = f"./qwen3_vl_video_vqa_fixed_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # æ£€æŸ¥æ•°æ®
    if not os.path.exists(TRAIN_DATA):
        print(f"âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {TRAIN_DATA}")
        return
    
    # 1. åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
    print("åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨...")
    try:
        model = AutoModelForImageTextToText.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        processor = AutoProcessor.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True
        )
        
        print("âœ… æ¨¡å‹å’Œå¤„ç†å™¨åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return
    
    # 2. åˆ›å»ºæ•°æ®é›†
    print("\nåˆ›å»ºæ•°æ®é›†...")
    train_dataset = VideoVQADataset(
        data_path=TRAIN_DATA,
        processor=processor,
        max_samples=20,  # é™åˆ¶æ ·æœ¬æ•°ç”¨äºæµ‹è¯•
        num_frames=8  # å‡å°‘å¸§æ•°é¿å…æ˜¾å­˜é—®é¢˜
    )
    
    if len(train_dataset) == 0:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
        return
    
    print(f"æ•°æ®é›†å¤§å°: {len(train_dataset)} ä¸ªæ ·æœ¬")
    
    # 3. åˆ›å»ºæ•°æ®æ”¶é›†å™¨
    data_collator = DataCollatorForVideoVQA(processor=processor)
    
    # 4. è®¾ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=3,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        learning_rate=2e-5,
        weight_decay=0.01,
        warmup_steps=10,
        logging_steps=5,
        save_steps=50,
        save_total_limit=2,
        remove_unused_columns=False,
        push_to_hub=False,
        report_to="none",
        dataloader_pin_memory=False,
        fp16=True,
        gradient_checkpointing=True,
    )
    
    # 5. åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
    )
    
    # 6. å¼€å§‹è®­ç»ƒ
    print(f"\nå¼€å§‹è®­ç»ƒï¼Œå…± {training_args.num_train_epochs} ä¸ªepoch")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    try:
        train_result = trainer.train()
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæŸå¤±: {train_result.training_loss:.4f}")
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        processor.save_pretrained(OUTPUT_DIR)
        print(f"æ¨¡å‹ä¿å­˜åˆ°: {OUTPUT_DIR}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_conversation_format():
    """æµ‹è¯•å¯¹è¯æ ¼å¼æ˜¯å¦æ­£ç¡®"""
    print("æµ‹è¯•å¯¹è¯æ ¼å¼...")
    
    # åŠ è½½å¤„ç†å™¨
    processor = AutoProcessor.from_pretrained(
        "Qwen/Qwen3-VL-2B-Instruct",
        trust_remote_code=True
    )
    
    # åˆ›å»ºæµ‹è¯•å›¾ç‰‡
    test_images = []
    for _ in range(8):
        test_image = Image.new('RGB', (640, 360), color='red')
        buffered = BytesIO()
        test_image.save(buffered, format="JPEG", quality=85)
        img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
        test_images.append(img_base64)
    
    # æ„å»ºprompt
    question = "What is the ego vehicle's action in the video?"
    answer = "The ego vehicle performs <driving_maneuver>Single_lane_driving</driving_maneuver> from <start_time>0</start_time> to <end_time>60</end_time> seconds."
    
    prompt = build_vqa_prompt(question, 60)
    
    # æ„å»ºå¯¹è¯
    conversation = prepare_conversation_format_training(test_images, prompt, answer)
    
    print(f"å¯¹è¯é•¿åº¦: {len(conversation)}")
    print(f"ç”¨æˆ·æ¶ˆæ¯å†…å®¹ç±»å‹: {[c['type'] for c in conversation[0]['content']]}")
    print(f"åŠ©æ‰‹æ¶ˆæ¯: {conversation[1]['content']}")
    
    # æµ‹è¯•å¤„ç†å™¨
    try:
        # æå–å›¾ç‰‡
        images = []
        for content in conversation[0]["content"]:
            if content["type"] == "image":
                # è§£ç base64å›¾ç‰‡
                image_data = base64.b64decode(content["image"])
                image = Image.open(BytesIO(image_data)).convert("RGB")
                images.append(image)
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text = processor.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=False
        )
        
        print(f"å¤„ç†åçš„æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # å¤„ç†è¾“å…¥
        inputs = processor(
            text=[text],
            images=[images],
            return_tensors="pt",
            padding=True
        )
        
        print("âœ… å¯¹è¯æ ¼å¼æµ‹è¯•é€šè¿‡")
        print(f"Input IDs shape: {inputs['input_ids'].shape}")
        print(f"Pixel values shape: {inputs['pixel_values'].shape}")
        
    except Exception as e:
        print(f"âŒ å¯¹è¯æ ¼å¼æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def validate_model_loading():
    """éªŒè¯æ¨¡å‹åŠ è½½å’ŒåŸºæœ¬æ¨ç†"""
    print("\néªŒè¯æ¨¡å‹åŠ è½½...")
    
    try:
        model = AutoModelForImageTextToText.from_pretrained(
            "Qwen/Qwen3-VL-2B-Instruct",
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        processor = AutoProcessor.from_pretrained(
            "Qwen/Qwen3-VL-2B-Instruct",
            trust_remote_code=True
        )
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•æ¨ç†
        test_images = []
        for _ in range(4):
            test_image = Image.new('RGB', (640, 360), color='blue')
            buffered = BytesIO()
            test_image.save(buffered, format="JPEG", quality=85)
            img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
            test_images.append(img_base64)
        
        # æ„å»ºprompt
        question = "What is in the image?"
        prompt = build_vqa_prompt(question, 60)
        
        # æ„å»ºå¯¹è¯
        conversation = prepare_conversation_format_inference(test_images, prompt)
        
        # æå–å›¾ç‰‡
        images = []
        for content in conversation[0]["content"]:
            if content["type"] == "image":
                image_data = base64.b64decode(content["image"])
                image = Image.open(BytesIO(image_data)).convert("RGB")
                images.append(image)
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text = processor.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # å¤„ç†è¾“å…¥
        inputs = processor(
            text=[text],
            images=[images],
            return_tensors="pt",
            padding=True
        )
        
        print(f"Input IDs shape: {inputs['input_ids'].shape}")
        print(f"Pixel values shape: {inputs['pixel_values'].shape}")
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                input_ids=inputs["input_ids"].to(model.device),
                attention_mask=inputs["attention_mask"].to(model.device),
                pixel_values=inputs["pixel_values"].to(model.device),
                max_new_tokens=50
            )
        
        decoded = processor.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ… æ¨ç†æµ‹è¯•é€šè¿‡")
        print(f"ç”Ÿæˆç»“æœ: {decoded[:100]}...")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # 1. æµ‹è¯•å¯¹è¯æ ¼å¼
    test_conversation_format()
    
    print("\n" + "=" * 60)
    
    # 2. éªŒè¯æ¨¡å‹åŠ è½½
    validate_model_loading()
    
    print("\n" + "=" * 60)
    
    # 3. æ£€æŸ¥GPU
    if not torch.cuda.is_available():
        print("âŒ éœ€è¦GPUè¿›è¡Œè®­ç»ƒ")
        print("å»ºè®®ä½¿ç”¨è‡³å°‘16GBæ˜¾å­˜çš„GPU")
    else:
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"\nğŸ® GPU: {gpu_name}")
        print(f"   æ˜¾å­˜: {gpu_memory:.1f} GB")
        
        if gpu_memory < 16:
            print("âš ï¸  è­¦å‘Š: å¯èƒ½éœ€è¦è¾ƒå¤šæ˜¾å­˜")
    
    # è¯¢é—®æ˜¯å¦ç»§ç»­
    response = input("\næ˜¯å¦å¼€å§‹è®­ç»ƒï¼Ÿ(y/n): ")
    if response.lower() == 'y':
        train_model()
    else:
        print("è®­ç»ƒå·²å–æ¶ˆ")