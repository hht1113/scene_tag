import os
import json
import torch
from typing import Dict, List, Optional
from tqdm import tqdm
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/root/workspace/video_vqa_inference.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class VideoVQAInference:
    """è§†é¢‘VQAæ¨ç†ç±»"""
    
    def __init__(self, model_path: str, test_file: str, output_dir: str):
        """
        åˆå§‹åŒ–æ¨ç†ç±»
        
        Args:
            model_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
            test_file: æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        self.model_path = model_path
        self.test_file = test_file
        self.output_dir = output_dir
        
        # æ£€æŸ¥è·¯å¾„
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        if not os.path.exists(test_file):
            raise FileNotFoundError(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # è®¾å¤‡è®¾ç½®
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ¨¡å‹å’Œtokenizer
        self.model = None
        self.tokenizer = None
        self.processor = None
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
            from peft import PeftModel, PeftConfig
            
            logger.info("å¼€å§‹åŠ è½½æ¨¡å‹...")
            
            # é¦–å…ˆåŠ è½½base model
            base_model_path = "Qwen/Qwen3-VL-2B-Instruct"
            logger.info(f"åŠ è½½base model: {base_model_path}")
            
            # åŠ è½½é…ç½®
            config = AutoConfig.from_pretrained(
                base_model_path,
                trust_remote_code=True
            )
            
            # è®¾ç½®æœ€å¤§æˆªæ–­åºåˆ—ä¸º4096
            config.max_position_embeddings = 4096
            config.model_max_length = 4096
            
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                base_model_path,
                trust_remote_code=True,
                model_max_length=4096
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            logger.info(f"TokenizeråŠ è½½å®Œæˆï¼Œpad_token: {self.tokenizer.pad_token}")
            
            # åŠ è½½base model
            self.model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                config=config,
                torch_dtype=torch.bfloat16 if self.device.type == "cuda" else torch.float32,
                device_map="auto" if self.device.type == "cuda" else None,
                trust_remote_code=True
            )
            
            # åŠ è½½LoRAæƒé‡
            logger.info(f"åŠ è½½LoRAæƒé‡: {self.model_path}")
            self.model = PeftModel.from_pretrained(self.model, self.model_path)
            
            # åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
            logger.info("åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹...")
            self.model = self.model.merge_and_unload()
            
            self.model.eval()
            logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def load_test_data(self) -> List[Dict]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        logger.info(f"åŠ è½½æµ‹è¯•æ•°æ®: {self.test_file}")
        
        with open(self.test_file, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        
        logger.info(f"åŠ è½½äº† {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
        
        # æ£€æŸ¥æ•°æ®ç»“æ„
        if test_data and len(test_data) > 0:
            sample = test_data[0]
            logger.info(f"æ ·æœ¬ç»“æ„: {list(sample.keys())}")
            logger.info(f"æ ·æœ¬ç¤ºä¾‹ - instruction: {sample.get('instruction', '')[:50]}...")
            logger.info(f"æ ·æœ¬ç¤ºä¾‹ - output: {sample.get('output', '')[:50]}...")
            
        return test_data
    
    def generate_prompt(self, sample: Dict) -> str:
        """ç”Ÿæˆæç¤ºè¯"""
        instruction = sample.get("instruction", "")
        input_text = sample.get("input", "")
        
        if input_text:
            return f"{instruction}\n{input_text}"
        else:
            return instruction
    
    def generate_response(self, prompt: str, videos: List[str], 
                         max_new_tokens: int = 512, 
                         temperature: float = 0.1) -> str:
        """
        ç”Ÿæˆå“åº”
        
        æ³¨æ„ï¼šQwen3-VLæ˜¯å¤šæ¨¡æ€æ¨¡å‹ï¼Œä½†åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸­æˆ‘ä»¬åªå¤„ç†æ–‡æœ¬
        è§†é¢‘ä¿¡æ¯åœ¨instructionä¸­é€šè¿‡<video>æ ‡è®°è¡¨ç¤º
        """
        try:
            # å‡†å¤‡å¯¹è¯
            messages = [
                {"role": "user", "content": prompt}
            ]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            try:
                text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
            except Exception as e:
                logger.warning(f"åº”ç”¨èŠå¤©æ¨¡æ¿å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æç¤º: {e}")
                text = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
            
            # Tokenizeè¾“å…¥
            inputs = self.tokenizer(
                text, 
                return_tensors="pt", 
                padding=True,
                truncation=True,
                max_length=4096 - max_new_tokens
            ).to(self.device)
            
            input_length = inputs.input_ids.shape[1]
            logger.debug(f"è¾“å…¥tokené•¿åº¦: {input_length}")
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§é•¿åº¦
            if input_length > 4096 - max_new_tokens:
                logger.warning(f"è¾“å…¥é•¿åº¦ {input_length} è¿‡é•¿ï¼Œå¯èƒ½è¢«æˆªæ–­")
            
            # ç”Ÿæˆå‚æ•°
            generation_kwargs = {
                "input_ids": inputs.input_ids,
                "attention_mask": inputs.attention_mask,
                "max_new_tokens": max_new_tokens,
                "temperature": temperature,
                "do_sample": temperature > 0.01,
                "top_p": 0.9 if temperature > 0.01 else None,
                "pad_token_id": self.tokenizer.pad_token_id if self.tokenizer.pad_token_id else self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
            }
            
            # ç”Ÿæˆ
            with torch.no_grad():
                try:
                    # ä½¿ç”¨generateæ–¹æ³•
                    generated_ids = self.model.generate(**generation_kwargs)
                    
                    # æå–ç”Ÿæˆçš„æ–‡æœ¬
                    generated_ids = generated_ids[0, inputs.input_ids.shape[1]:]
                    response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
                    
                    # æ¸…ç†å“åº”
                    response = response.strip()
                    
                    # ç§»é™¤å¯èƒ½çš„åœæ­¢åºåˆ—
                    stop_sequences = ["<|im_end|>", "</s>", "<|endoftext|>", "\n\n\n"]
                    for stop_seq in stop_sequences:
                        if response.endswith(stop_seq):
                            response = response[:-len(stop_seq)].strip()
                    
                    return response
                    
                except Exception as e:
                    logger.error(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                    return f"ç”Ÿæˆé”™è¯¯: {str(e)}"
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå“åº”å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return f"Error: {str(e)}"
    
    def batch_inference(self, test_data: List[Dict]) -> List[Dict]:
        """
        æ‰¹é‡æ¨ç†
        
        Args:
            test_data: æµ‹è¯•æ•°æ®
            
        Returns:
            åŒ…å«é¢„æµ‹ç»“æœçš„æ•°æ®åˆ—è¡¨
        """
        results = []
        
        logger.info(f"å¼€å§‹æ¨ç†ï¼Œå…± {len(test_data)} ä¸ªæ ·æœ¬")
        
        for i, sample in enumerate(tqdm(test_data, desc="æ¨ç†è¿›åº¦")):
            try:
                # æå–æ•°æ®
                instruction = sample.get("instruction", "")
                input_text = sample.get("input", "")
                gt_answer = sample.get("output", "")
                video_paths = sample.get("videos", [])
                
                if not instruction:
                    logger.warning(f"æ ·æœ¬ {i} æ²¡æœ‰instructionå­—æ®µ")
                    continue
                
                # æ£€æŸ¥è§†é¢‘è·¯å¾„
                video_exists = all(os.path.exists(v) for v in video_paths) if video_paths else False
                
                # ç”Ÿæˆæç¤ºè¯
                prompt = self.generate_prompt(sample)
                
                # æ£€æŸ¥<video>æ ‡è®°
                has_video_tag = "<video>" in instruction
                video_tag_count = instruction.count("<video>")
                video_count = len(video_paths) if video_paths else 0
                
                # ç”Ÿæˆå“åº”
                pred_answer = self.generate_response(
                    prompt, 
                    video_paths,
                    max_new_tokens=512,
                    temperature=0.1
                )
                
                # æ„å»ºç»“æœ
                result = {
                    "sample_id": i,
                    "instruction": instruction,
                    "input": input_text,
                    "ground_truth": gt_answer,
                    "prediction": pred_answer,
                    "video_paths": video_paths,
                    "video_exists": video_exists,
                    "has_video_tag": has_video_tag,
                    "video_tag_count": video_tag_count,
                    "video_count": video_count
                }
                
                results.append(result)
                
                # æ¯10ä¸ªæ ·æœ¬è®°å½•ä¸€æ¬¡
                if (i + 1) % 10 == 0:
                    logger.info(f"å·²å¤„ç† {i + 1}/{len(test_data)} ä¸ªæ ·æœ¬")
                    # æ˜¾ç¤ºæœ€è¿‘ä¸€ä¸ªæ ·æœ¬çš„ç¤ºä¾‹
                    logger.info(f"  ç¤ºä¾‹ {i} - è¾“å…¥: {instruction[:50]}...")
                    logger.info(f"  ç¤ºä¾‹ {i} - é¢„æµ‹: {pred_answer[:50]}...")
                
                # æ¸…ç†æ˜¾å­˜
                if torch.cuda.is_available() and (i + 1) % 20 == 0:
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                logger.error(f"å¤„ç†æ ·æœ¬ {i} å¤±è´¥: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
                
                # æ·»åŠ é”™è¯¯æ ·æœ¬
                error_result = {
                    "sample_id": i,
                    "instruction": sample.get("instruction", ""),
                    "input": sample.get("input", ""),
                    "ground_truth": sample.get("output", ""),
                    "prediction": f"ERROR: {str(e)}",
                    "video_paths": sample.get("videos", []),
                    "video_exists": False,
                    "has_video_tag": False,
                    "video_tag_count": 0,
                    "video_count": 0,
                    "error": str(e)
                }
                results.append(error_result)
        
        successful_count = len([r for r in results if 'ERROR' not in r.get('prediction', '')])
        logger.info(f"æ¨ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {successful_count}/{len(test_data)} ä¸ªæ ·æœ¬")
        return results
    
    def save_results(self, results: List[Dict]):
        """ä¿å­˜æ¨ç†ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_file = os.path.join(self.output_dir, f"inference_results_{timestamp}.json")
        
        result_data = {
            "model_path": self.model_path,
            "test_file": self.test_file,
            "timestamp": timestamp,
            "total_samples": len(results),
            "successful_samples": len([r for r in results if 'ERROR' not in r.get('prediction', '')]),
            "failed_samples": len([r for r in results if 'ERROR' in r.get('prediction', '')]),
            "results": results
        }
        
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2, separators=(',', ': '))
        
        logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜: {detailed_file}")
        
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡
        successful_results = [r for r in results if 'ERROR' not in r.get('prediction', '')]
        if successful_results:
            # è®¡ç®—å¹³å‡é•¿åº¦
            avg_gt_len = sum(len(str(r.get('ground_truth', ''))) for r in successful_results) / len(successful_results)
            avg_pred_len = sum(len(str(r.get('prediction', ''))) for r in successful_results) / len(successful_results)
            
            # ç»Ÿè®¡è§†é¢‘æ ‡è®°ä¸€è‡´æ€§
            consistent_count = sum(1 for r in successful_results if r.get('video_tag_count', 0) == r.get('video_count', 0))
            video_exists_count = sum(1 for r in successful_results if r.get('video_exists', False))
            
            stats = {
                "avg_ground_truth_length": avg_gt_len,
                "avg_prediction_length": avg_pred_len,
                "video_tag_consistent_samples": consistent_count,
                "video_exists_samples": video_exists_count
            }
            
            stats_file = os.path.join(self.output_dir, f"stats_{timestamp}.txt")
            with open(stats_file, 'w', encoding='utf-8') as f:
                f.write(f"æ¨¡å‹è·¯å¾„: {self.model_path}\n")
                f.write(f"æµ‹è¯•æ–‡ä»¶: {self.test_file}\n")
                f.write(f"æ—¶é—´æˆ³: {timestamp}\n")
                f.write(f"æ€»æ ·æœ¬æ•°: {len(results)}\n")
                f.write(f"æˆåŠŸæ¨ç†: {len(successful_results)}\n")
                f.write(f"å¤±è´¥æ¨ç†: {len(results) - len(successful_results)}\n")
                f.write(f"æˆåŠŸç‡: {len(successful_results)/len(results)*100:.2f}%\n")
                f.write(f"å¹³å‡GTé•¿åº¦: {avg_gt_len:.2f} å­—ç¬¦\n")
                f.write(f"å¹³å‡é¢„æµ‹é•¿åº¦: {avg_pred_len:.2f} å­—ç¬¦\n")
                f.write(f"è§†é¢‘æ ‡è®°ä¸€è‡´æ ·æœ¬: {consistent_count}/{len(successful_results)} ({(consistent_count/len(successful_results))*100:.2f}%)\n")
                f.write(f"è§†é¢‘æ–‡ä»¶å­˜åœ¨æ ·æœ¬: {video_exists_count}/{len(successful_results)} ({(video_exists_count/len(successful_results))*100:.2f}%)\n")
            
            logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")
        
        return detailed_file
    
    def run(self):
        """è¿è¡Œæ¨ç†æµç¨‹"""
        logger.info("=" * 60)
        logger.info("è§†é¢‘VQAæ¨ç†å¼€å§‹")
        logger.info("=" * 60)
        logger.info(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
        logger.info(f"æµ‹è¯•æ–‡ä»¶: {self.test_file}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info(f"è®¾å¤‡: {self.device}")
        logger.info(f"æœ€å¤§æˆªæ–­åºåˆ—: 4096")
        logger.info("=" * 60)
        
        # 1. åŠ è½½æ¨¡å‹
        logger.info("æ­¥éª¤ 1/3: åŠ è½½æ¨¡å‹...")
        self.load_model()
        
        # 2. åŠ è½½æµ‹è¯•æ•°æ®
        logger.info("æ­¥éª¤ 2/3: åŠ è½½æµ‹è¯•æ•°æ®...")
        test_data = self.load_test_data()
        
        if not test_data:
            logger.error("æµ‹è¯•æ•°æ®ä¸ºç©ºï¼Œé€€å‡ºæ¨ç†")
            return None, None
        
        # 3. è¿è¡Œæ¨ç†
        logger.info("æ­¥éª¤ 3/3: è¿è¡Œæ¨ç†...")
        results = self.batch_inference(test_data)
        
        # 4. ä¿å­˜ç»“æœ
        logger.info("ä¿å­˜ç»“æœ...")
        result_file = self.save_results(results)
        
        # 5. æ‰“å°æ€»ç»“
        self.print_summary(results, result_file)
        
        return results, result_file
    
    def print_summary(self, results: List[Dict], result_file: str):
        """æ‰“å°æ€»ç»“ä¿¡æ¯"""
        successful_results = [r for r in results if 'ERROR' not in r.get('prediction', '')]
        failed_results = [r for r in results if 'ERROR' in r.get('prediction', '')]
        
        print("\n" + "=" * 60)
        print("æ¨ç†å®Œæˆæ€»ç»“")
        print("=" * 60)
        print(f"æ€»æ ·æœ¬æ•°: {len(results)}")
        print(f"æˆåŠŸæ¨ç†: {len(successful_results)}")
        print(f"å¤±è´¥æ¨ç†: {len(failed_results)}")
        print(f"æˆåŠŸç‡: {len(successful_results)/len(results)*100:.2f}%")
        
        if successful_results:
            # ç»Ÿè®¡è§†é¢‘æ ‡è®°ä¸€è‡´æ€§
            consistent_count = sum(1 for r in successful_results if r.get('video_tag_count', 0) == r.get('video_count', 0))
            print(f"è§†é¢‘æ ‡è®°ä¸€è‡´æ€§: {consistent_count}/{len(successful_results)} ({(consistent_count/len(successful_results))*100:.2f}%)")
            
            # è§†é¢‘æ–‡ä»¶å­˜åœ¨æ€§
            video_exists_count = sum(1 for r in successful_results if r.get('video_exists', False))
            print(f"è§†é¢‘æ–‡ä»¶å­˜åœ¨: {video_exists_count}/{len(successful_results)} ({(video_exists_count/len(successful_results))*100:.2f}%)")
            
            # å¹³å‡é•¿åº¦
            avg_gt_len = sum(len(str(r.get('ground_truth', ''))) for r in successful_results) / len(successful_results)
            avg_pred_len = sum(len(str(r.get('prediction', ''))) for r in successful_results) / len(successful_results)
            print(f"å¹³å‡GTé•¿åº¦: {avg_gt_len:.2f} å­—ç¬¦")
            print(f"å¹³å‡é¢„æµ‹é•¿åº¦: {avg_pred_len:.2f} å­—ç¬¦")
        
        print("\n" + "=" * 60)
        print("é¢„æµ‹ç»“æœç¤ºä¾‹")
        print("=" * 60)
        
        for i, result in enumerate(successful_results[:3]):
            print(f"\nç¤ºä¾‹ {i + 1}:")
            print(f"  æŒ‡ä»¤: {result.get('instruction', '')[:80]}...")
            print(f"  çœŸå®ç­”æ¡ˆ: {result.get('ground_truth', '')[:80]}...")
            print(f"  é¢„æµ‹ç­”æ¡ˆ: {result.get('prediction', '')[:80]}...")
            print(f"  è§†é¢‘å­˜åœ¨: {result.get('video_exists', False)}")
            print(f"  è§†é¢‘æ ‡è®°: {result.get('has_video_tag', False)}")
        
        print("\n" + "=" * 60)
        print("è¾“å‡ºæ–‡ä»¶")
        print("=" * 60)
        print(f"ç»“æœæ–‡ä»¶: {result_file}")
        
        # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
        if os.path.exists(result_file):
            size_mb = os.path.getsize(result_file) / (1024 * 1024)
            print(f"æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")
        
        # æŸ¥æ‰¾ç»Ÿè®¡æ–‡ä»¶
        import glob
        stats_files = glob.glob(os.path.join(self.output_dir, "stats_*.txt"))
        if stats_files:
            latest_stats = max(stats_files, key=os.path.getctime)
            print(f"ç»Ÿè®¡æ–‡ä»¶: {latest_stats}")
        
        print("=" * 60)
        print("æ¨ç†å®Œæˆ!")
        print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    # å›ºå®šå‚æ•°
    MODEL_PATH = "/root/workspace/LLaMA-Factory/saves/Qwen3-VL-2B-Instruct/lora/train_lora_2026-01-03-11-13-37-"
    TEST_FILE = "/root/workspace/llama_factory_vqa_dataset/llama_factory_vqa_20251231_210010/test.json"
    OUTPUT_DIR = "/root/workspace/video_vqa_inference_results"
    
    print("=" * 60)
    print("è§†é¢‘VQAæ¨ç†è„šæœ¬")
    print("=" * 60)
    print(f"æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print(f"æµ‹è¯•æ–‡ä»¶: {TEST_FILE}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print("=" * 60)
    
    # æ£€æŸ¥æµ‹è¯•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(TEST_FILE):
        print(f"é”™è¯¯: æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {TEST_FILE}")
        print("è¯·æ£€æŸ¥æµ‹è¯•æ–‡ä»¶è·¯å¾„")
        return
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(MODEL_PATH):
        print(f"è­¦å‘Š: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MODEL_PATH}")
        print("å°è¯•åŠ è½½base modelå’ŒLoRAæƒé‡...")
    
    # åˆ›å»ºæ¨ç†å™¨
    try:
        inference = VideoVQAInference(
            model_path=MODEL_PATH,
            test_file=TEST_FILE,
            output_dir=OUTPUT_DIR
        )
        
        # è¿è¡Œæ¨ç†
        results, result_file = inference.run()
        
        if results and result_file:
            print(f"\nâœ… æ¨ç†å®Œæˆ!")
            print(f"ğŸ“ ç»“æœæ–‡ä»¶: {result_file}")
            
            # æ˜¾ç¤ºç»“æœæ–‡ä»¶ä½ç½®
            print(f"\nğŸ“Š æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ç»“æœ:")
            print(f"  cat {result_file} | head -n 100")
            print(f"  python -c \"import json; data=json.load(open('{result_file}')); print(f'æ€»æ ·æœ¬æ•°: {len(data.get(\\\"results\\\", []))}')\"")
            
    except Exception as e:
        print(f"\nâŒ æ¨ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    # è®¾ç½®æœ€å¤§åˆ†å‰²å¤§å°
    torch.cuda.empty_cache()
    
    main()